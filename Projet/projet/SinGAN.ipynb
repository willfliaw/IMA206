{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;font-size:1.5em\">\n",
    "\n",
    "**Authors: Anastasiia Karpova, Valentin Abribat, William Liaw**\n",
    "\n",
    "Academic report presented to Télécom Paris as an activity of the course Photographie computationelle / Méthodes par patchs (IMA206).\n",
    "\n",
    "**Palaiseau**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ABSTRACT**\n",
    "\n",
    "This document presents the codebase and accompanying documentation for Project 8 of Group 11, focusing on the re-implementation of the SinGAN architecture. SinGAN, originally introduced by Shaham, Dekel, and Michaeli (2019), is a generative model designed to learn from a single natural image. The re-implementation is based on:\n",
    "\n",
    "    Shaham, T. R., Dekel, T., & Michaeli, T. (2019). SinGAN: Learning a Generative Model from a Single Natural Image. arXiv. https://doi.org/10.48550/arxiv.1905.01164\n",
    "\n",
    "and its official source code available at:\n",
    "\n",
    "    Tamarott. (n.d.). GitHub - tamarott/SinGAN: Official PyTorch implementation of the paper: “SinGAN: Learning a Generative Model from a Single Natural Image.” GitHub. https://github.com/tamarott/SinGAN\n",
    "\n",
    "Additionally, we conducted various experiments, focusing primarily on evaluating different padding functions for images and optimizing the cost function for random sample generation by the generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TABLE OF CONTENTS**\n",
    "\n",
    "1. Modules imports\n",
    "2. Hyperparameter definitions\n",
    "   1. Workspace\n",
    "   2. Load, input, save configurations\n",
    "   3. Networks\n",
    "   4. Pyramid\n",
    "   5. Optimization\n",
    "   6. Fréchet Induction Distance\n",
    "3. Auxiliary functions\n",
    "4. SinGAN\n",
    "5. Models\n",
    "6. Training\n",
    "   1. Training functions\n",
    "   2. Main\n",
    "7. Random Samples\n",
    "   1. Main\n",
    "8.  SIFID\n",
    "    1.  SIFID functions\n",
    "    2.  Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISCLAIMER**\n",
    "\n",
    "The sections *Modules imports*, *Hyperparameter definitions*, *Auxiliary functions*, *SinGAN*, *Models* should be executed at the beginning of every execution of the present work, otherwise the code may yield errors or unexpected behaviors. Appart from those sections, the following sections can be executed independently:\n",
    "\n",
    "- Training: trains a model\n",
    "- Random Samples: generates random samples using the architecture in a directory (by default: `TrainedModels`)\n",
    "- SIFID: using the SIFID metric, evaluates fake images in a given directory (by default `Output/RandomSamples/<img_name>/gen_start_scale=X`) with respects to a real image (by default: `Input/Images/birds.png`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout all the methodology of the present academic work, the following modules were used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from matplotlib.pyplot import imread\n",
    "from torchvision import models\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import v2\n",
    "from tqdm import tqdm\n",
    "from torchaudio.functional import frechet_distance\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this code implementation, the following hyperparameters were required:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_cuda = False  # disables cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\" if not_cuda else \"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, input, save configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG_path = \"\"  # path to netG (to continue training)\n",
    "netD_path = \"\"  # path to netD (to continue training)\n",
    "manualSeed = None  # manual seed\n",
    "nc_z = 3  # noise # channels\n",
    "nc_im = 3  # image # channels\n",
    "out = \"Output\"  # output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if manualSeed is None:\n",
    "    manualSeed = random.randint(1, 10000)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "print(\"Random Seed: \", manualSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfc = 32  # number of feature components\n",
    "min_nfc = 32  # number of minimal feature components\n",
    "kernel_size = 3  # kernel size\n",
    "num_layer = 5  # number of layers\n",
    "stride = 1  # stride\n",
    "padd_size = 0  # net pad size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image is resized if it's bigger than max_size\n",
    "# Pyramid of images scales: 1, scale_factor, scale_factor^2, ... scale_factor^n\n",
    "scale_factor = 0.75  # pyramid scale factor\n",
    "noise_amp_init = 0.1  # addative noise cont weight\n",
    "min_size = 25  # image minimal size at the smallest scale\n",
    "max_size = 250  # image maximal size at the biggest scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niter = 1000  # number of epochs to train per scale\n",
    "gamma = 0.1  # scheduler gamma\n",
    "lr_g = 0.0005  # learning rate\n",
    "lr_d = 0.0005  # learning rate\n",
    "beta1 = 0.5  # beta1 for adam\n",
    "Gsteps = 1  # Generator inner steps\n",
    "Dsteps = 3  # Discriminator inner steps\n",
    "lambda_grad = 0.1  # gradient penelty weight\n",
    "alpha = 10  # reconstruction loss weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fréchet Induction Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 11  # patch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions were utilized to perform various tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descale(x):\n",
    "    \"\"\"\n",
    "    Descale a tensor from the range [-1, 1] to [0, 1].\n",
    "    \"\"\"\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)\n",
    "\n",
    "\n",
    "def scale(x):\n",
    "    \"\"\"\n",
    "    Scale a tensor from the range [0, 1] to [-1, 1].\n",
    "    \"\"\"\n",
    "    out = (x - 0.5) * 2\n",
    "    return out.clamp(-1, 1)\n",
    "\n",
    "\n",
    "def convert_image_np(img):\n",
    "    \"\"\"\n",
    "    Convert a PyTorch tensor to a NumPy array with values in the range [0, 1].\n",
    "    \"\"\"\n",
    "    return np.clip(descale(img).squeeze(0).cpu().numpy().transpose(1, 2, 0), 0, 1)\n",
    "\n",
    "\n",
    "def calc_gradient_penalty(netD, real_data, fake_data, lambda_, device):\n",
    "    \"\"\"\n",
    "    Calculate the gradient penalty for WGAN-GP\n",
    "    \"\"\"\n",
    "    alpha = torch.rand(1, 1)\n",
    "    alpha = alpha.expand(real_data.size())\n",
    "    alpha = alpha.to(device)\n",
    "\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "\n",
    "    interpolates = interpolates.to(device)\n",
    "    interpolates = torch.autograd.Variable(interpolates, requires_grad=True)\n",
    "\n",
    "    disc_interpolates = netD(interpolates)\n",
    "\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=disc_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_\n",
    "    return gradient_penalty\n",
    "\n",
    "\n",
    "def upsampling(im, sx, sy):\n",
    "    \"\"\"\n",
    "    Upsample an image to the specified size using bilinear interpolation.\n",
    "    \"\"\"\n",
    "    m = nn.Upsample(size=[round(sx), round(sy)], mode=\"bilinear\", align_corners=True)\n",
    "    return m(im)\n",
    "\n",
    "\n",
    "def get_mu_sigma(sample, device, patch_size=patch_size):\n",
    "    \"\"\"\n",
    "    Calculate the mean and covariance of patches from an image tensor.\n",
    "    \"\"\"\n",
    "    P = (\n",
    "        sample.squeeze(0)\n",
    "        .permute(1, 2, 0)\n",
    "        .unfold(0, patch_size, 1)\n",
    "        .unfold(1, patch_size, 1)\n",
    "    )\n",
    "    P = torch.reshape(P, (-1, 3 * patch_size * patch_size)).T\n",
    "\n",
    "    mu = torch.mean(P, dim=1).to(device)\n",
    "    sigma = torch.cov(P).to(device)\n",
    "\n",
    "    return mu, sigma\n",
    "\n",
    "\n",
    "def get_mean_border(image):\n",
    "    \"\"\"\n",
    "    Calculate the mean pixel value of the borders of an image.\n",
    "    \"\"\"\n",
    "    top_border = image[:, 0, :]\n",
    "    bottom_border = image[:, -1, :]\n",
    "    left_border = image[:, :, 0]\n",
    "    right_border = image[:, :, -1]\n",
    "\n",
    "    border_pixels = torch.cat(\n",
    "        [top_border, left_border, right_border, bottom_border], dim=1\n",
    "    )\n",
    "\n",
    "    return border_pixels.mean()\n",
    "\n",
    "\n",
    "def get_padding(mode_pad, pad_image, pad_noise, image=None):\n",
    "    \"\"\"\n",
    "    Get padding layers for noise and image based on the specified mode.\n",
    "    \"\"\"\n",
    "    if mode_pad == 0:\n",
    "        m_noise = nn.ZeroPad2d(pad_noise)\n",
    "        m_image = nn.ZeroPad2d(pad_image)\n",
    "    elif mode_pad == 1:\n",
    "        pad_const = get_mean_border(image.squeeze(0))\n",
    "        m_noise = nn.ConstantPad2d(pad_noise, pad_const)\n",
    "        m_image = nn.ConstantPad2d(pad_image, pad_const)\n",
    "    elif mode_pad == 2:\n",
    "        m_noise = nn.ReplicationPad2d(pad_noise)\n",
    "        m_image = nn.ReplicationPad2d(pad_image)\n",
    "    elif mode_pad == 3:\n",
    "        m_noise = nn.CircularPad2d(pad_noise)\n",
    "        m_image = nn.CircularPad2d(pad_image)\n",
    "    return m_noise, m_image\n",
    "\n",
    "\n",
    "def get_padding_singan(mode_pad, pad_image, image=None):\n",
    "    \"\"\"\n",
    "    Get padding layers for image based on the specified mode.\n",
    "    \"\"\"\n",
    "    if mode_pad == 0:\n",
    "        m_image = nn.ZeroPad2d(pad_image)\n",
    "    elif mode_pad == 1:\n",
    "        pad_const = get_mean_border(image.squeeze(0))\n",
    "        m_image = nn.ConstantPad2d(pad_image, pad_const)\n",
    "    elif mode_pad == 2:\n",
    "        m_image = nn.ReplicationPad2d(pad_image)\n",
    "    elif mode_pad == 3:\n",
    "        m_image = nn.CircularPad2d(pad_image)\n",
    "    return m_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SinGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SinGAN_generate(\n",
    "    Gs,\n",
    "    Zs,\n",
    "    reals,\n",
    "    NoiseAmp,\n",
    "    mode,\n",
    "    input_name,\n",
    "    in_s=None,\n",
    "    scale_v=1,\n",
    "    scale_h=1,\n",
    "    n=0,\n",
    "    gen_start_scale=0,\n",
    "    num_samples=500,\n",
    "    mode_pad=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function generates fake random samples using the SinGAN architecture\n",
    "    \"\"\"\n",
    "    if in_s is None:\n",
    "        in_s = torch.zeros(reals[0].shape, device=device)\n",
    "    images_cur = []\n",
    "    for G, Z_opt, noise_amp in zip(Gs, Zs, NoiseAmp):\n",
    "        pad1 = int(((kernel_size - 1) * num_layer) / 2)\n",
    "        m = get_padding_singan(mode_pad, pad1, reals[0])\n",
    "        nzx = (Z_opt.shape[2] - pad1 * 2) * scale_v\n",
    "        nzy = (Z_opt.shape[3] - pad1 * 2) * scale_h\n",
    "\n",
    "        images_prev = images_cur\n",
    "        images_cur = []\n",
    "\n",
    "        for i in range(0, num_samples, 1):\n",
    "            if n == 0:\n",
    "                z_curr = torch.randn(\n",
    "                    1,\n",
    "                    1,\n",
    "                    round(nzx),\n",
    "                    round(nzy),\n",
    "                    device=device,\n",
    "                )\n",
    "                z_curr = z_curr.expand(1, 3, z_curr.shape[2], z_curr.shape[3])\n",
    "            else:\n",
    "                z_curr = torch.randn(\n",
    "                    1,\n",
    "                    nc_z,\n",
    "                    round(nzx),\n",
    "                    round(nzy),\n",
    "                    device=device,\n",
    "                )\n",
    "\n",
    "            z_curr = m(z_curr)\n",
    "\n",
    "            if images_prev == []:\n",
    "                I_prev = m(in_s)\n",
    "            else:\n",
    "                I_prev = images_prev[i]\n",
    "                I_prev = v2.functional.resize(\n",
    "                    I_prev,\n",
    "                    (\n",
    "                        round(nzx),\n",
    "                        round(nzy),\n",
    "                    ),\n",
    "                )\n",
    "                I_prev = m(I_prev)\n",
    "\n",
    "            if n < gen_start_scale:\n",
    "                z_curr = Z_opt\n",
    "\n",
    "            z_in = noise_amp * z_curr + I_prev\n",
    "            I_curr = G(z_in.detach(), I_prev)\n",
    "\n",
    "            if n == len(reals) - 1:\n",
    "                if mode == \"train\" or mode == \"random_samples\":\n",
    "                    dir_to_save = f\"{out}/RandomSamples/{input_name[:-4]}/gen_start_scale={gen_start_scale}\"\n",
    "                elif mode == \"random_samples_arbitraty_size\":\n",
    "                    dir_to_save = f\"{out}/RandomSamples_ArbitrerySizes/{input_name[:-4]}/scale_v={scale_v}_scale_h={scale_h}\"\n",
    "\n",
    "                try:\n",
    "                    os.makedirs(dir_to_save)\n",
    "                except OSError:\n",
    "                    pass\n",
    "\n",
    "                plt.imsave(\n",
    "                    f\"{dir_to_save}/{i}.png\",\n",
    "                    convert_image_np(I_curr.detach()),\n",
    "                    vmin=0,\n",
    "                    vmax=1,\n",
    "                )\n",
    "            images_cur.append(I_curr)\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Sequential):\n",
    "    \"\"\"\n",
    "    Convolutional Block from SinGAN\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel, out_channel, kernel_size, padd, stride):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.add_module(\n",
    "            \"conv\",\n",
    "            nn.Conv2d(\n",
    "                in_channel,\n",
    "                out_channel,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padd,\n",
    "                stride=stride,\n",
    "            ),\n",
    "        ),\n",
    "        self.add_module(\"norm\", nn.BatchNorm2d(out_channel)),\n",
    "        self.add_module(\"LeakyRelu\", nn.LeakyReLU(0.2, inplace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generator Block from SinGAN\n",
    "    \"\"\"\n",
    "    def __init__(self, nc_im, nfc, min_nfc, kernel_size, padd_size, num_layer):\n",
    "        super(Generator, self).__init__()\n",
    "        # Receives an \"image\"\n",
    "        self.head = ConvBlock(nc_im, nfc, kernel_size, padd_size, 1)\n",
    "        self.body = nn.Sequential()\n",
    "        for i in range(num_layer - 2):\n",
    "            # Each convolutional block halves the number of channels\n",
    "            N = math.floor(nfc / (2 ** (i + 1)))\n",
    "            block = ConvBlock(\n",
    "                max(2 * N, min_nfc),\n",
    "                max(N, min_nfc),\n",
    "                kernel_size,\n",
    "                padd_size,\n",
    "                1,\n",
    "            )\n",
    "            self.body.add_module(f\"block{i + 1}\", block)\n",
    "        # Outputs an image\n",
    "        self.tail = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                max(N, min_nfc),\n",
    "                nc_im,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=1,\n",
    "                padding=padd_size,\n",
    "            ),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.head(x)\n",
    "        x = self.body(x)\n",
    "        x = self.tail(x)\n",
    "        ind = int((y.shape[2] - x.shape[2]) / 2)\n",
    "        y = y[:, :, ind : (y.shape[2] - ind), ind : (y.shape[3] - ind)]\n",
    "        return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator Block from SinGAN\n",
    "    \"\"\"\n",
    "    def __init__(self, nc_im, nfc, min_nfc, kernel_size, padd_size, num_layer):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Receives an \"image\"\n",
    "        self.head = ConvBlock(nc_im, nfc, kernel_size, padd_size, 1)\n",
    "        self.body = nn.Sequential()\n",
    "        for i in range(num_layer - 2):\n",
    "            # Each convolutional block halves the number of channels\n",
    "            N = nfc // (2 ** (i + 1))\n",
    "            block = ConvBlock(\n",
    "                max(2 * N, min_nfc),\n",
    "                max(N, min_nfc),\n",
    "                kernel_size,\n",
    "                padd_size,\n",
    "                1,\n",
    "            )\n",
    "            self.body.add_module(f\"block{i + 1}\", block)\n",
    "        # Outputs a label\n",
    "        self.tail = nn.Conv2d(\n",
    "            max(N, min_nfc),\n",
    "            1,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=padd_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.head(x)\n",
    "        x = self.body(x)\n",
    "        x = self.tail(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    \"\"\"\n",
    "    Performs weights initialization on a given model\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv2d\") != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find(\"Norm\") != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "def init_models(\n",
    "    netG_path,\n",
    "    netD_path,\n",
    "    nc_im,\n",
    "    nfc,\n",
    "    min_nfc,\n",
    "    kernel_size,\n",
    "    padd_size,\n",
    "    num_layer,\n",
    "    device,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Initialize models for the SinGAN architecture\n",
    "    \"\"\"\n",
    "    netG = Generator(nc_im, nfc, min_nfc, kernel_size, padd_size, num_layer).to(device)\n",
    "    netG.apply(weights_init)\n",
    "    if netG_path != \"\":\n",
    "        netG.load_state_dict(torch.load(netG_path))\n",
    "    netD = Discriminator(nc_im, nfc, min_nfc, kernel_size, padd_size, num_layer).to(\n",
    "        device\n",
    "    )\n",
    "    netD.apply(weights_init)\n",
    "    if netD_path != \"\":\n",
    "        netD.load_state_dict(torch.load(netD_path))\n",
    "    if verbose:\n",
    "        print(netG)\n",
    "        print(netD)\n",
    "    return netD, netG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_concat(\n",
    "    Gs, Zs, reals, NoiseAmp, in_s, mode, nc_z, m_noise, m_image, scale_factor\n",
    "):\n",
    "    \"\"\"\n",
    "    Progressively enlarges, adds noise and introduces a given image to Generators\n",
    "    in a concatenated fashion\n",
    "    \"\"\"\n",
    "    G_z = in_s\n",
    "    if len(Gs) > 0:\n",
    "        if mode == \"rand\":\n",
    "            count = 0\n",
    "            pad_noise = int(((kernel_size - 1) * num_layer) / 2)\n",
    "            for G, Z_opt, real_curr, real_next, noise_amp in zip(\n",
    "                Gs, Zs, reals, reals[1:], NoiseAmp\n",
    "            ):\n",
    "                if count == 0:\n",
    "                    z = torch.randn(\n",
    "                        1,\n",
    "                        1,\n",
    "                        Z_opt.shape[2] - 2 * pad_noise,\n",
    "                        Z_opt.shape[3] - 2 * pad_noise,\n",
    "                        device=device,\n",
    "                    )\n",
    "                    z = z.expand(1, 3, z.shape[2], z.shape[3])\n",
    "                else:\n",
    "                    z = torch.randn(\n",
    "                        1,\n",
    "                        nc_z,\n",
    "                        Z_opt.shape[2] - 2 * pad_noise,\n",
    "                        Z_opt.shape[3] - 2 * pad_noise,\n",
    "                        device=device,\n",
    "                    )\n",
    "                z = m_noise(z)\n",
    "                G_z = G_z[:, :, 0 : real_curr.shape[2], 0 : real_curr.shape[3]]\n",
    "                G_z = m_image(G_z)\n",
    "                z_in = noise_amp * z + G_z\n",
    "                G_z = G(z_in.detach(), G_z)\n",
    "                G_z = v2.functional.resize(\n",
    "                    G_z,\n",
    "                    (\n",
    "                        math.ceil(G_z.shape[2] / scale_factor),\n",
    "                        math.ceil(G_z.shape[3] / scale_factor),\n",
    "                    ),\n",
    "                )\n",
    "                G_z = G_z[:, :, 0 : real_next.shape[2], 0 : real_next.shape[3]]\n",
    "                count += 1\n",
    "        if mode == \"rec\":\n",
    "            count = 0\n",
    "            for G, Z_opt, real_curr, real_next, noise_amp in zip(\n",
    "                Gs, Zs, reals, reals[1:], NoiseAmp\n",
    "            ):\n",
    "                G_z = G_z[:, :, 0 : real_curr.shape[2], 0 : real_curr.shape[3]]\n",
    "                G_z = m_image(G_z)\n",
    "                z_in = noise_amp * Z_opt + G_z\n",
    "                G_z = G(z_in.detach(), G_z)\n",
    "                G_z = v2.functional.resize(\n",
    "                    G_z,\n",
    "                    (\n",
    "                        math.ceil(G_z.shape[2] / scale_factor),\n",
    "                        math.ceil(G_z.shape[3] / scale_factor),\n",
    "                    ),\n",
    "                )\n",
    "                G_z = G_z[:, :, 0 : real_next.shape[2], 0 : real_next.shape[3]]\n",
    "                count += 1\n",
    "\n",
    "    return G_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_scale(\n",
    "    netD,\n",
    "    netG,\n",
    "    reals,\n",
    "    Gs,\n",
    "    Zs,\n",
    "    in_s,\n",
    "    NoiseAmp,\n",
    "    kernel_size,\n",
    "    num_layer,\n",
    "    stride,\n",
    "    alpha,\n",
    "    nc_z,\n",
    "    lr_d,\n",
    "    lr_g,\n",
    "    beta1,\n",
    "    scale_num,\n",
    "    noise_amp_init,\n",
    "    scale_factor,\n",
    "    mode_pad,\n",
    "    dir_to_save,\n",
    "    draw_plots=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a single step (scale) of the SinGAN pyramid, with a WGAN with\n",
    "    Gradient Penalty fashion\n",
    "    \"\"\"\n",
    "    real = reals[len(Gs)]\n",
    "    nzx = real.shape[2]\n",
    "    nzy = real.shape[3]\n",
    "    pad_noise = round((kernel_size - 1) * num_layer / 2)\n",
    "    pad_image = round((kernel_size - 1) * num_layer / 2)\n",
    "    m_noise, m_image = get_padding(mode_pad, pad_image, pad_noise, real)\n",
    "\n",
    "    z_opt = torch.zeros((1, nc_z, nzx, nzy), device=device)\n",
    "    z_opt = m_noise(z_opt)\n",
    "\n",
    "    # setup optimizer\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=lr_d, betas=(beta1, 0.999))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=lr_g, betas=(beta1, 0.999))\n",
    "    schedulerD = optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer=optimizerD, milestones=[1600], gamma=gamma\n",
    "    )\n",
    "    schedulerG = optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer=optimizerG, milestones=[1600], gamma=gamma\n",
    "    )\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = nn.MSELoss()\n",
    "\n",
    "    errD2plot = []\n",
    "    errG2plot = []\n",
    "    D_real2plot = []\n",
    "    D_fake2plot = []\n",
    "    z_opt2plot = []\n",
    "    xplot = []\n",
    "    for epoch in (pbar := tqdm(range(niter))):\n",
    "        pbar.set_postfix_str(f\"scale: {scale_num}\")\n",
    "        if Gs == []:\n",
    "            z_opt = torch.randn(1, 1, nzx, nzy, device=device)\n",
    "            z_opt = m_noise(z_opt.expand(1, 3, nzx, nzy))\n",
    "            noise_ = torch.randn(1, 1, nzx, nzy, device=device)\n",
    "            noise_ = m_noise(noise_.expand(1, 3, nzx, nzy))\n",
    "        else:\n",
    "            noise_ = torch.randn(1, nc_z, nzx, nzy, device=device)\n",
    "            noise_ = m_noise(noise_)\n",
    "\n",
    "        # (1) Update D network: maximize D(x) + D(G(z))\n",
    "        for j in range(Dsteps):\n",
    "            # train with real\n",
    "            optimizerD.zero_grad()\n",
    "            output = netD(real)\n",
    "            errD_real = -output.mean()\n",
    "            errD_real.backward(retain_graph=True)\n",
    "            D_x = -errD_real.item()\n",
    "\n",
    "            # train with fake\n",
    "            if j == 0 and epoch == 0:\n",
    "                if Gs == []:\n",
    "                    prev = torch.zeros((1, nc_z, nzx, nzy), device=device)\n",
    "                    in_s = torch.zeros((1, nc_z, nzx, nzy), device=device)\n",
    "                    prev = m_image(prev)\n",
    "                    z_prev = torch.zeros((1, nc_z, nzx, nzy), device=device)\n",
    "                    z_prev = m_noise(z_prev)\n",
    "                    noise_amp = 1\n",
    "                else:\n",
    "                    prev = draw_concat(\n",
    "                        Gs,\n",
    "                        Zs,\n",
    "                        reals,\n",
    "                        NoiseAmp,\n",
    "                        in_s,\n",
    "                        \"rand\",\n",
    "                        nc_z,\n",
    "                        m_noise,\n",
    "                        m_image,\n",
    "                        scale_factor,\n",
    "                    )\n",
    "                    prev = m_image(prev)\n",
    "                    z_prev = draw_concat(\n",
    "                        Gs,\n",
    "                        Zs,\n",
    "                        reals,\n",
    "                        NoiseAmp,\n",
    "                        in_s,\n",
    "                        \"rec\",\n",
    "                        nc_z,\n",
    "                        m_noise,\n",
    "                        m_image,\n",
    "                        scale_factor,\n",
    "                    )\n",
    "                    RMSE = torch.sqrt(criterion(real, z_prev))\n",
    "                    noise_amp = noise_amp_init * RMSE\n",
    "                    z_prev = m_image(z_prev)\n",
    "            else:\n",
    "                prev = draw_concat(\n",
    "                    Gs,\n",
    "                    Zs,\n",
    "                    reals,\n",
    "                    NoiseAmp,\n",
    "                    in_s,\n",
    "                    \"rand\",\n",
    "                    nc_z,\n",
    "                    m_noise,\n",
    "                    m_image,\n",
    "                    scale_factor,\n",
    "                )\n",
    "                prev = m_image(prev)\n",
    "\n",
    "            if Gs == []:\n",
    "                noise = noise_\n",
    "            else:\n",
    "                noise = noise_amp * noise_ + prev\n",
    "\n",
    "            fake = netG(noise.detach(), prev)\n",
    "            output = netD(fake.detach())\n",
    "            errD_fake = output.mean()\n",
    "            errD_fake.backward(retain_graph=True)\n",
    "            D_G_z = output.mean().item()\n",
    "\n",
    "            gradient_penalty = calc_gradient_penalty(\n",
    "                netD, real, fake, lambda_grad, device\n",
    "            )\n",
    "            gradient_penalty.backward()\n",
    "\n",
    "            errD = errD_real + errD_fake + gradient_penalty\n",
    "            optimizerD.step()\n",
    "\n",
    "        # (2) Update G network: maximize D(G(z))\n",
    "        for j in range(Gsteps):\n",
    "            optimizerG.zero_grad()\n",
    "            if j == 0 and epoch == 0:\n",
    "                if Gs == []:\n",
    "                    prev = torch.zeros((1, nc_z, nzx, nzy), device=device)\n",
    "                    in_s = torch.zeros((1, nc_z, nzx, nzy), device=device)\n",
    "                    prev = m_image(prev)\n",
    "                    noise_amp = 1\n",
    "                else:\n",
    "                    prev = draw_concat(\n",
    "                        Gs,\n",
    "                        Zs,\n",
    "                        reals,\n",
    "                        NoiseAmp,\n",
    "                        in_s,\n",
    "                        \"rand\",\n",
    "                        nc_z,\n",
    "                        m_noise,\n",
    "                        m_image,\n",
    "                        scale_factor,\n",
    "                    )\n",
    "                    prev = m_image(prev)\n",
    "                    z_prev = draw_concat(\n",
    "                        Gs,\n",
    "                        Zs,\n",
    "                        reals,\n",
    "                        NoiseAmp,\n",
    "                        in_s,\n",
    "                        \"rec\",\n",
    "                        nc_z,\n",
    "                        m_noise,\n",
    "                        m_image,\n",
    "                        scale_factor,\n",
    "                    )\n",
    "                    RMSE = torch.sqrt(criterion(real, z_prev))\n",
    "                    noise_amp = noise_amp_init * RMSE\n",
    "                    z_prev = m_image(z_prev)\n",
    "\n",
    "            if Gs == []:\n",
    "                noise = noise_\n",
    "            else:\n",
    "                noise = noise_amp * noise_ + prev\n",
    "\n",
    "            fake = netG(noise.detach(), prev)\n",
    "            output = netD(fake)\n",
    "            errG = -output.mean()\n",
    "            errG.backward(retain_graph=True)\n",
    "            if alpha != 0:\n",
    "                Z_opt = noise_amp * z_opt + z_prev\n",
    "                rec_loss = alpha * loss(netG(Z_opt.detach(), z_prev), real)\n",
    "                rec_loss.backward(retain_graph=True)\n",
    "                rec_loss = rec_loss.detach()\n",
    "            else:\n",
    "                Z_opt = z_opt\n",
    "                rec_loss = 0\n",
    "\n",
    "            optimizerG.step()\n",
    "\n",
    "        if epoch % round(niter / 50) == 0 or epoch == (niter - 1):\n",
    "            xplot.append(epoch)\n",
    "            errD2plot.append(errD.detach().cpu())\n",
    "            errG2plot.append((errG.detach() + rec_loss).cpu())\n",
    "            D_real2plot.append(D_x)\n",
    "            D_fake2plot.append(D_G_z)\n",
    "            z_opt2plot.append(rec_loss.cpu())\n",
    "\n",
    "        if epoch % round(niter / 4) == 0 or epoch == (niter - 1):\n",
    "            plt.imsave(\n",
    "                f\"{dir_to_save}/{scale_num}/fake_sample.png\",\n",
    "                convert_image_np(fake.detach()),\n",
    "                vmin=0,\n",
    "                vmax=1,\n",
    "            )\n",
    "            plt.imsave(\n",
    "                f\"{dir_to_save}/{scale_num}/G(z_opt).png\",\n",
    "                convert_image_np(netG(Z_opt.detach(), z_prev).detach()),\n",
    "                vmin=0,\n",
    "                vmax=1,\n",
    "            )\n",
    "\n",
    "            torch.save(z_opt, f\"{dir_to_save}/{scale_num}/z_opt.pt\")\n",
    "\n",
    "        schedulerD.step()\n",
    "        schedulerG.step()\n",
    "\n",
    "    if draw_plots:\n",
    "        fig = plt.figure(figsize=(10, 6), dpi=80)\n",
    "\n",
    "        plt.title(f\"Discriminator loss - scale {scale_num}\", fontsize=22)\n",
    "\n",
    "        plt.plot(xplot, errD2plot, label=\"Discriminator loss\")\n",
    "        plt.plot(xplot, D_real2plot, label=\"Discriminator loss (real)\")\n",
    "        plt.plot(xplot, D_fake2plot, label=\"Discriminator loss (fake)\")\n",
    "\n",
    "        plt.gca().spines[[\"top\", \"right\"]].set_alpha(0)\n",
    "        plt.gca().spines[[\"bottom\", \"left\"]].set_alpha(0.3)\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"{dir_to_save}/{scale_num}/discriminator_loss_plot.png\")\n",
    "        plt.show()\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 6), dpi=80)\n",
    "\n",
    "        plt.title(f\"Generator loss - scale {scale_num}\", fontsize=22)\n",
    "\n",
    "        plt.plot(xplot, errG2plot, label=\"Generator loss\")\n",
    "        plt.plot(xplot, z_opt2plot, label=\"Reconstruction loss\")\n",
    "\n",
    "        plt.gca().spines[[\"top\", \"right\"]].set_alpha(0)\n",
    "        plt.gca().spines[[\"bottom\", \"left\"]].set_alpha(0.3)\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"{dir_to_save}/{scale_num}/generator_loss_plot.png\")\n",
    "        plt.show()\n",
    "\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(15, 5), dpi=80)\n",
    "\n",
    "        axs[0].imshow(convert_image_np(fake.detach()))\n",
    "        axs[0].set_title(\"Fake sample\")\n",
    "        axs[0].axis(\"off\")\n",
    "\n",
    "        axs[1].imshow(convert_image_np(netG(Z_opt.detach(), z_prev).detach()))\n",
    "        axs[1].set_title(\"G(z_opt)\")\n",
    "        axs[1].axis(\"off\")\n",
    "\n",
    "        axs[2].imshow(convert_image_np(real))\n",
    "        axs[2].set_title(\"Real\")\n",
    "        axs[2].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{dir_to_save}/{scale_num}/imgs_plot.png\")\n",
    "        plt.show()\n",
    "\n",
    "    torch.save(netG.state_dict(), f\"{dir_to_save}/{scale_num}/netG.pt\")\n",
    "    torch.save(netD.state_dict(), f\"{dir_to_save}/{scale_num}/netD.pt\")\n",
    "    torch.save(z_opt, f\"{dir_to_save}/{scale_num}/z_opt.pt\")\n",
    "\n",
    "    return z_opt, in_s, netG, noise_amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_scale_frechet_distance(\n",
    "    netG,\n",
    "    reals,\n",
    "    Gs,\n",
    "    Zs,\n",
    "    in_s,\n",
    "    NoiseAmp,\n",
    "    kernel_size,\n",
    "    num_layer,\n",
    "    stride,\n",
    "    alpha,\n",
    "    nc_z,\n",
    "    lr_d,\n",
    "    lr_g,\n",
    "    beta1,\n",
    "    scale_num,\n",
    "    noise_amp_init,\n",
    "    scale_factor,\n",
    "    mode_pad,\n",
    "    dir_to_save,\n",
    "    draw_plots=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a single step (scale) of the SinGAN pyramid, with a Fréchet\n",
    "    Distance from the patch distribution as a cost function\n",
    "    \"\"\"\n",
    "    real = reals[len(Gs)]\n",
    "    nzx = real.shape[2]\n",
    "    nzy = real.shape[3]\n",
    "    pad_noise = round((kernel_size - 1) * num_layer / 2)\n",
    "    pad_image = round((kernel_size - 1) * num_layer / 2)\n",
    "    m_noise, m_image = get_padding(mode_pad, pad_image, pad_noise, real)\n",
    "\n",
    "    z_opt = torch.zeros((1, nc_z, nzx, nzy), device=device)\n",
    "    z_opt = m_noise(z_opt)\n",
    "\n",
    "    # setup optimizer\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=lr_g, betas=(beta1, 0.999))\n",
    "    schedulerG = optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer=optimizerG, milestones=[1600], gamma=gamma\n",
    "    )\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = nn.MSELoss()\n",
    "\n",
    "    errG2plot = []\n",
    "    z_opt2plot = []\n",
    "    xplot = []\n",
    "    for epoch in (pbar := tqdm(range(niter))):\n",
    "        pbar.set_postfix_str(f\"scale: {scale_num}\")\n",
    "        if Gs == []:\n",
    "            z_opt = torch.randn(1, 1, nzx, nzy, device=device)\n",
    "            z_opt = m_noise(z_opt.expand(1, 3, nzx, nzy))\n",
    "            noise_ = torch.randn(1, 1, nzx, nzy, device=device)\n",
    "            noise_ = m_noise(noise_.expand(1, 3, nzx, nzy))\n",
    "        else:\n",
    "            noise_ = torch.randn(1, nc_z, nzx, nzy, device=device)\n",
    "            noise_ = m_noise(noise_)\n",
    "\n",
    "        # Update G network: maximize FID(G(z))\n",
    "        for j in range(Gsteps):\n",
    "            optimizerG.zero_grad()\n",
    "            if j == 0 and epoch == 0:\n",
    "                if Gs == []:\n",
    "                    prev = torch.zeros((1, nc_z, nzx, nzy), device=device)\n",
    "                    in_s = torch.zeros((1, nc_z, nzx, nzy), device=device)\n",
    "                    prev = m_image(prev)\n",
    "                    z_prev = torch.zeros((1, nc_z, nzx, nzy), device=device)\n",
    "                    z_prev = m_noise(z_prev)\n",
    "                    noise_amp = 1\n",
    "                else:\n",
    "                    prev = draw_concat(\n",
    "                        Gs,\n",
    "                        Zs,\n",
    "                        reals,\n",
    "                        NoiseAmp,\n",
    "                        in_s,\n",
    "                        \"rand\",\n",
    "                        nc_z,\n",
    "                        m_noise,\n",
    "                        m_image,\n",
    "                        scale_factor,\n",
    "                    )\n",
    "                    prev = m_image(prev)\n",
    "                    z_prev = draw_concat(\n",
    "                        Gs,\n",
    "                        Zs,\n",
    "                        reals,\n",
    "                        NoiseAmp,\n",
    "                        in_s,\n",
    "                        \"rec\",\n",
    "                        nc_z,\n",
    "                        m_noise,\n",
    "                        m_image,\n",
    "                        scale_factor,\n",
    "                    )\n",
    "                    RMSE = torch.sqrt(criterion(real, z_prev))\n",
    "                    noise_amp = noise_amp_init * RMSE\n",
    "                    z_prev = m_image(z_prev)\n",
    "\n",
    "            if Gs == []:\n",
    "                noise = noise_\n",
    "            else:\n",
    "                noise = noise_amp * noise_ + prev\n",
    "\n",
    "            fake = netG(noise.detach(), prev)\n",
    "            mu_real, sigma_real = get_mu_sigma(real, device)\n",
    "            mu_fake, sigma_fake = get_mu_sigma(fake, device)\n",
    "            errG = frechet_distance(mu_real, sigma_real, mu_fake, sigma_fake)\n",
    "            errG.backward(retain_graph=True)\n",
    "            if alpha != 0:\n",
    "                Z_opt = noise_amp * z_opt + z_prev\n",
    "                rec_loss = alpha * loss(netG(Z_opt.detach(), z_prev), real)\n",
    "                rec_loss.backward(retain_graph=True)\n",
    "                rec_loss = rec_loss.detach()\n",
    "            else:\n",
    "                Z_opt = z_opt\n",
    "                rec_loss = 0\n",
    "\n",
    "            optimizerG.step()\n",
    "\n",
    "        if epoch % round(niter / 50) == 0 or epoch == (niter - 1):\n",
    "            xplot.append(epoch)\n",
    "            errG2plot.append((errG.detach() + rec_loss).cpu())\n",
    "            z_opt2plot.append(rec_loss.cpu())\n",
    "\n",
    "        if epoch % round(niter / 4) == 0 or epoch == (niter - 1):\n",
    "            plt.imsave(\n",
    "                f\"{dir_to_save}/{scale_num}/fake_sample.png\",\n",
    "                convert_image_np(fake.detach()),\n",
    "                vmin=0,\n",
    "                vmax=1,\n",
    "            )\n",
    "            plt.imsave(\n",
    "                f\"{dir_to_save}/{scale_num}/G(z_opt).png\",\n",
    "                convert_image_np(netG(Z_opt.detach(), z_prev).detach()),\n",
    "                vmin=0,\n",
    "                vmax=1,\n",
    "            )\n",
    "\n",
    "            torch.save(z_opt, f\"{dir_to_save}/{scale_num}/z_opt.pt\")\n",
    "\n",
    "        schedulerG.step()\n",
    "\n",
    "    if draw_plots:\n",
    "        fig = plt.figure(figsize=(10, 6), dpi=80)\n",
    "\n",
    "        plt.title(f\"Generator loss - scale {scale_num}\", fontsize=22)\n",
    "\n",
    "        plt.plot(xplot, errG2plot, label=\"Generator loss\")\n",
    "        plt.plot(xplot, z_opt2plot, label=\"Reconstruction loss\")\n",
    "\n",
    "        plt.gca().spines[[\"top\", \"right\"]].set_alpha(0)\n",
    "        plt.gca().spines[[\"bottom\", \"left\"]].set_alpha(0.3)\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"{dir_to_save}/{scale_num}/generator_loss_plot.png\")\n",
    "        plt.show()\n",
    "\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(15, 5), dpi=80)\n",
    "\n",
    "        axs[0].imshow(convert_image_np(fake.detach()))\n",
    "        axs[0].set_title(\"Fake sample\")\n",
    "        axs[0].axis(\"off\")\n",
    "\n",
    "        axs[1].imshow(convert_image_np(netG(Z_opt.detach(), z_prev).detach()))\n",
    "        axs[1].set_title(\"G(z_opt)\")\n",
    "        axs[1].axis(\"off\")\n",
    "\n",
    "        axs[2].imshow(convert_image_np(real))\n",
    "        axs[2].set_title(\"Real\")\n",
    "        axs[2].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{dir_to_save}/{scale_num}/imgs_plot.png\")\n",
    "        plt.show()\n",
    "\n",
    "    torch.save(netG.state_dict(), f\"{dir_to_save}/{scale_num}/netG.pt\")\n",
    "    torch.save(z_opt, f\"{dir_to_save}/{scale_num}/z_opt.pt\")\n",
    "\n",
    "    return z_opt, in_s, netG, noise_amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    real,\n",
    "    stop_scale,\n",
    "    scale_factor,\n",
    "    nfc,\n",
    "    min_nfc,\n",
    "    dir_to_save,\n",
    "    netG_path,\n",
    "    netD_path,\n",
    "    noise_amp_init,\n",
    "    device,\n",
    "    frechet=True,\n",
    "    mode_pad=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs de training of SinGAN\n",
    "    \"\"\"\n",
    "    Gs = []\n",
    "    Zs = []\n",
    "    reals = []\n",
    "    NoiseAmp = []\n",
    "    in_s = 0\n",
    "    scale_num = 0\n",
    "    nfc_prev = 0\n",
    "\n",
    "    # Create pyramid\n",
    "    scales = np.logspace(\n",
    "        start=stop_scale, stop=0, num=stop_scale + 1, base=scale_factor\n",
    "    )\n",
    "    for scale in scales:\n",
    "        curr_real = v2.functional.resize(\n",
    "            real, (math.ceil(scale * real.shape[2]), math.ceil(scale * real.shape[3]))\n",
    "        )\n",
    "        reals.append(curr_real)\n",
    "\n",
    "    # Iterating on the pyramid\n",
    "    for scale_num, curr_real in enumerate(reals):\n",
    "        # Every 4 steps on the pyramid the number of features components doubles\n",
    "        nfc = min(nfc * (2 ** math.floor(scale_num / 4)), 128)\n",
    "        min_nfc = min(min_nfc * (2 ** math.floor(scale_num / 4)), 128)\n",
    "\n",
    "        try:\n",
    "            os.makedirs(f\"{dir_to_save}/{scale_num}\")\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "        plt.imsave(\n",
    "            f\"{dir_to_save}/{scale_num}/real_scale.png\",\n",
    "            convert_image_np(curr_real),\n",
    "            vmin=0,\n",
    "            vmax=1,\n",
    "        )\n",
    "\n",
    "        if frechet:\n",
    "            _, G_curr = init_models(\n",
    "                netG_path,\n",
    "                \"\",\n",
    "                nc_im,\n",
    "                nfc,\n",
    "                min_nfc,\n",
    "                kernel_size,\n",
    "                padd_size,\n",
    "                num_layer,\n",
    "                device,\n",
    "            )\n",
    "        else:\n",
    "            D_curr, G_curr = init_models(\n",
    "                netG_path,\n",
    "                netD_path,\n",
    "                nc_im,\n",
    "                nfc,\n",
    "                min_nfc,\n",
    "                kernel_size,\n",
    "                padd_size,\n",
    "                num_layer,\n",
    "                device,\n",
    "            )\n",
    "\n",
    "        # Models with the same number of features (4 neighbor steps) are initialized\n",
    "        # with the same weights\n",
    "        if nfc_prev == nfc:\n",
    "            G_curr.load_state_dict(torch.load(f\"{dir_to_save}/{scale_num - 1}/netG.pt\"))\n",
    "            if not frechet:\n",
    "                D_curr.load_state_dict(\n",
    "                    torch.load(f\"{dir_to_save}/{scale_num - 1}/netD.pt\")\n",
    "                )\n",
    "\n",
    "        if frechet:\n",
    "            z_curr, in_s, G_curr, noise_amp = train_single_scale_frechet_distance(\n",
    "                G_curr,\n",
    "                reals,\n",
    "                Gs,\n",
    "                Zs,\n",
    "                in_s,\n",
    "                NoiseAmp,\n",
    "                kernel_size,\n",
    "                num_layer,\n",
    "                stride,\n",
    "                alpha,\n",
    "                nc_z,\n",
    "                lr_d,\n",
    "                lr_g,\n",
    "                beta1,\n",
    "                scale_num,\n",
    "                noise_amp_init,\n",
    "                scale_factor,\n",
    "                mode_pad,\n",
    "                dir_to_save,\n",
    "            )\n",
    "        else:\n",
    "            z_curr, in_s, G_curr, noise_amp = train_single_scale(\n",
    "                D_curr,\n",
    "                G_curr,\n",
    "                reals,\n",
    "                Gs,\n",
    "                Zs,\n",
    "                in_s,\n",
    "                NoiseAmp,\n",
    "                kernel_size,\n",
    "                num_layer,\n",
    "                stride,\n",
    "                alpha,\n",
    "                nc_z,\n",
    "                lr_d,\n",
    "                lr_g,\n",
    "                beta1,\n",
    "                scale_num,\n",
    "                noise_amp_init,\n",
    "                scale_factor,\n",
    "                mode_pad,\n",
    "                dir_to_save,\n",
    "            )\n",
    "\n",
    "        # Persistency\n",
    "        G_curr = G_curr.requires_grad_(False).eval()\n",
    "        if not frechet:\n",
    "            D_curr = D_curr.requires_grad_(False).eval()\n",
    "        Gs.append(G_curr)\n",
    "        Zs.append(z_curr)\n",
    "        NoiseAmp.append(noise_amp)\n",
    "        torch.save(Zs, f\"{dir_to_save}/Zs.pt\")\n",
    "        torch.save(Gs, f\"{dir_to_save}/Gs.pt\")\n",
    "        torch.save(reals, f\"{dir_to_save}/reals.pt\")\n",
    "        torch.save(NoiseAmp, f\"{dir_to_save}/NoiseAmp.pt\")\n",
    "\n",
    "        nfc_prev = nfc\n",
    "\n",
    "        if frechet:\n",
    "            del G_curr\n",
    "        else:\n",
    "            del D_curr, G_curr\n",
    "\n",
    "    return Gs, Zs, reals, NoiseAmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"Input/Images\"  # input image dir\n",
    "input_name = \"birds.png\"  # input image name\n",
    "mode = \"train\"  # task to be done\"\n",
    "frechet = False\n",
    "mode_pad = 0\n",
    "num_samples = 500\n",
    "\n",
    "# for random_samples:\n",
    "gen_start_scale = 0  # generation start scale\n",
    "\n",
    "dir_to_save = (\n",
    "    f\"TrainedModels/{input_name[:-4]}/scale_factor={scale_factor},alpha={alpha}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(dir_to_save):\n",
    "    print(\"Trained model already exist\")\n",
    "else:\n",
    "    try:\n",
    "        os.makedirs(dir_to_save)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    real = read_image(f\"{input_dir}/{input_name}\")\n",
    "    real = v2.functional.to_dtype(real, dtype=torch.float32, scale=True)\n",
    "    real = real[:nc_im]\n",
    "    real = scale(real)\n",
    "    real = real.unsqueeze(0)\n",
    "    real = real.to(device)\n",
    "\n",
    "    num_scales = math.ceil(math.log(min_size / min(real.shape[2:]), scale_factor)) + 1\n",
    "    scale_to_stop = math.ceil(\n",
    "        math.log(\n",
    "            min(max_size, max(real.shape[2:])) / max(real.shape[2:]),\n",
    "            scale_factor,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    stop_scale = num_scales - scale_to_stop\n",
    "    # First scale of the pyramid is the image's original size, or max_size\n",
    "    scale0 = min(max_size / max(real.shape[2:]), 1)\n",
    "    real = v2.functional.resize(\n",
    "        real, (math.ceil(scale0 * real.shape[2]), math.ceil(scale0 * real.shape[3]))\n",
    "    )\n",
    "\n",
    "    # Corrected scale factor\n",
    "    corrected_scale_factor = (min_size / min(real.shape[2:])) ** (1 / (stop_scale))\n",
    "\n",
    "    Gs, Zs, reals, NoiseAmp = train(\n",
    "        real,\n",
    "        stop_scale,\n",
    "        corrected_scale_factor,\n",
    "        nfc,\n",
    "        min_nfc,\n",
    "        dir_to_save,\n",
    "        netG_path,\n",
    "        netD_path,\n",
    "        noise_amp_init,\n",
    "        device,\n",
    "        frechet,\n",
    "        mode_pad,\n",
    "    )\n",
    "    SinGAN_generate(\n",
    "        Gs,\n",
    "        Zs,\n",
    "        reals,\n",
    "        NoiseAmp,\n",
    "        mode,\n",
    "        input_name,\n",
    "        gen_start_scale=gen_start_scale,\n",
    "        num_samples=num_samples,\n",
    "        mode_pad=mode_pad,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"Input/Images\"  # input image dir\n",
    "input_name = \"birds.png\"\n",
    "mode = \"random_samples\"  # random_samples | random_samples_arbitrary_sizes\n",
    "num_samples = 500\n",
    "\n",
    "# for random_samples:\n",
    "gen_start_scale = 0  # generation start scale\n",
    "\n",
    "# for random_samples_arbitrary_sizes:\n",
    "scale_h = 1.5  # horizontal resize factor for random samples\n",
    "scale_v = 1  # vertical resize factor for random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gs = []\n",
    "Zs = []\n",
    "reals = []\n",
    "NoiseAmp = []\n",
    "if mode == \"random_samples\":\n",
    "    dir_to_save = f\"{out}/RandomSamples/{input_name[:-4]}/gen_start_scale={gen_start_scale}\"  # random_samples\n",
    "elif mode == \"random_samples_arbitrary_sizes\":\n",
    "    dir_to_save = f\"{out}/RandomSamples_ArbitrerySizes/{input_name[:-4]}/scale_v={scale_v}_scale_h={scale_h}\"  # random_samples_arbitraty_size\n",
    "dir_to_save_train = (\n",
    "    f\"TrainedModels/{input_name[:-4]}/scale_factor={scale_factor},alpha={alpha}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(dir_to_save):\n",
    "    if mode == \"random_samples\":\n",
    "        print(\n",
    "            f\"random samples for image {input_name}, start scale={gen_start_scale}, already exist\"\n",
    "        )\n",
    "    elif mode == \"random_samples_arbitrary_sizes\":\n",
    "        print(\n",
    "            f\"random samples for image {input_name} at size: scale_h={scale_h}, scale_v={scale_v}, already exist\"\n",
    "        )\n",
    "else:\n",
    "    try:\n",
    "        os.makedirs(dir_to_save)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    if os.path.exists(dir_to_save_train):\n",
    "        Gs = torch.load(f\"{dir_to_save_train}/Gs.pt\")\n",
    "        Zs = torch.load(f\"{dir_to_save_train}/Zs.pt\")\n",
    "        reals = torch.load(f\"{dir_to_save_train}/reals.pt\")\n",
    "        NoiseAmp = torch.load(f\"{dir_to_save_train}/NoiseAmp.pt\")\n",
    "\n",
    "    if mode == \"random_samples\":\n",
    "        real = reals[gen_start_scale]\n",
    "        real_down = upsampling(real, real.shape[2], real.shape[3])\n",
    "        if gen_start_scale == 0:\n",
    "            in_s = torch.full(real_down.shape, 0, device=device)\n",
    "        else:\n",
    "            in_s = upsampling(real_down, real_down.shape[2], real_down.shape[3])\n",
    "\n",
    "        SinGAN_generate(\n",
    "            Gs,\n",
    "            Zs,\n",
    "            reals,\n",
    "            NoiseAmp,\n",
    "            mode,\n",
    "            input_name,\n",
    "            gen_start_scale=gen_start_scale,\n",
    "            num_samples=num_samples,\n",
    "            mode_pad=mode_pad,\n",
    "        )\n",
    "\n",
    "    elif mode == \"random_samples_arbitrary_sizes\":\n",
    "        real = reals[gen_start_scale]\n",
    "        real_down = upsampling(real, scale_v * real.shape[2], scale_h * real.shape[3])\n",
    "        if gen_start_scale == 0:\n",
    "            in_s = torch.full(real_down.shape, 0, device=device)\n",
    "        else:\n",
    "            in_s = upsampling(real_down, real_down.shape[2], real_down.shape[3])\n",
    "\n",
    "        SinGAN_generate(\n",
    "            Gs,\n",
    "            Zs,\n",
    "            reals,\n",
    "            NoiseAmp,\n",
    "            mode,\n",
    "            input_name,\n",
    "            in_s=in_s,\n",
    "            scale_v=scale_v,\n",
    "            scale_h=scale_h,\n",
    "            num_samples=num_samples,\n",
    "            mode_pad=mode_pad,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIFID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIFID functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionV3(nn.Module):\n",
    "    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n",
    "\n",
    "    # Index of default block of inception to return,\n",
    "    # corresponds to output of final average pooling\n",
    "    DEFAULT_BLOCK_INDEX = 3\n",
    "\n",
    "    # Maps feature dimensionality to their output blocks indices\n",
    "    BLOCK_INDEX_BY_DIM = {\n",
    "        64: 0,  # First max pooling features\n",
    "        192: 1,  # Second max pooling featurs\n",
    "        768: 2,  # Pre-aux classifier features\n",
    "        2048: 3,  # Final average pooling features\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_blocks=[DEFAULT_BLOCK_INDEX],\n",
    "        resize_input=False,\n",
    "        normalize_input=True,\n",
    "        requires_grad=False,\n",
    "    ):\n",
    "        \"\"\"Build pretrained InceptionV3\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        output_blocks : list of int\n",
    "            Indices of blocks to return features of. Possible values are:\n",
    "                - 0: corresponds to output of first max pooling\n",
    "                - 1: corresponds to output of second max pooling\n",
    "                - 2: corresponds to output which is fed to aux classifier\n",
    "                - 3: corresponds to output of final average pooling\n",
    "        resize_input : bool\n",
    "            If true, bilinearly resizes input to width and height 299 before\n",
    "            feeding input to model. As the network without fully connected\n",
    "            layers is fully convolutional, it should be able to handle inputs\n",
    "            of arbitrary size, so resizing might not be strictly needed\n",
    "        normalize_input : bool\n",
    "            If true, scales the input from range (0, 1) to the range the\n",
    "            pretrained Inception network expects, namely (-1, 1)\n",
    "        requires_grad : bool\n",
    "            If true, parameters of the model require gradient. Possibly useful\n",
    "            for finetuning the network\n",
    "        \"\"\"\n",
    "        super(InceptionV3, self).__init__()\n",
    "\n",
    "        self.resize_input = resize_input\n",
    "        self.normalize_input = normalize_input\n",
    "        self.output_blocks = sorted(output_blocks)\n",
    "        self.last_needed_block = max(output_blocks)\n",
    "\n",
    "        assert self.last_needed_block <= 3, \"Last possible output block index is 3\"\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "\n",
    "        inception = models.inception_v3(weights=\"IMAGENET1K_V1\")\n",
    "\n",
    "        # Block 0: input to maxpool1\n",
    "        block0 = [\n",
    "            inception.Conv2d_1a_3x3,\n",
    "            inception.Conv2d_2a_3x3,\n",
    "            inception.Conv2d_2b_3x3,\n",
    "        ]\n",
    "\n",
    "        self.blocks.append(nn.Sequential(*block0))\n",
    "\n",
    "        # Block 1: maxpool1 to maxpool2\n",
    "        if self.last_needed_block >= 1:\n",
    "            block1 = [\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "                inception.Conv2d_3b_1x1,\n",
    "                inception.Conv2d_4a_3x3,\n",
    "            ]\n",
    "            self.blocks.append(nn.Sequential(*block1))\n",
    "\n",
    "        # Block 2: maxpool2 to aux classifier\n",
    "        if self.last_needed_block >= 2:\n",
    "            block2 = [\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "                inception.Mixed_5b,\n",
    "                inception.Mixed_5c,\n",
    "                inception.Mixed_5d,\n",
    "                inception.Mixed_6a,\n",
    "                inception.Mixed_6b,\n",
    "                inception.Mixed_6c,\n",
    "                inception.Mixed_6d,\n",
    "                inception.Mixed_6e,\n",
    "            ]\n",
    "            self.blocks.append(nn.Sequential(*block2))\n",
    "\n",
    "        # Block 3: aux classifier to final avgpool\n",
    "        if self.last_needed_block >= 3:\n",
    "            block3 = [\n",
    "                inception.Mixed_7a,\n",
    "                inception.Mixed_7b,\n",
    "                inception.Mixed_7c,\n",
    "            ]\n",
    "            self.blocks.append(nn.Sequential(*block3))\n",
    "\n",
    "        if self.last_needed_block >= 4:\n",
    "            block4 = [nn.AdaptiveAvgPool2d(output_size=(1, 1))]\n",
    "            self.blocks.append(nn.Sequential(*block4))\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "\n",
    "    def forward(self, inp):\n",
    "        \"\"\"Get Inception feature maps\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inp : torch.autograd.Variable\n",
    "            Input tensor of shape Bx3xHxW. Values are expected to be in\n",
    "            range (0, 1)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List of torch.autograd.Variable, corresponding to the selected output\n",
    "        block, sorted ascending by index\n",
    "        \"\"\"\n",
    "        outp = []\n",
    "        x = inp\n",
    "\n",
    "        if self.resize_input:\n",
    "            x = F.upsample(x, size=(299, 299), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        if self.normalize_input:\n",
    "            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n",
    "\n",
    "        for idx, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "            if idx in self.output_blocks:\n",
    "                outp.append(x)\n",
    "\n",
    "            if idx == self.last_needed_block:\n",
    "                break\n",
    "\n",
    "        return outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(files, model, batch_size=1, dims=64, cuda=False, verbose=False):\n",
    "    \"\"\"Calculates the activations of the pool_3 layer for all images.\n",
    "\n",
    "    Params:\n",
    "    -- files       : List of image files paths\n",
    "    -- model       : Instance of inception model\n",
    "    -- batch_size  : Batch size of images for the model to process at once.\n",
    "                     Make sure that the number of samples is a multiple of\n",
    "                     the batch size, otherwise some samples are ignored. This\n",
    "                     behavior is retained to match the original FID score\n",
    "                     implementation.\n",
    "    -- dims        : Dimensionality of features returned by Inception\n",
    "    -- cuda        : If set to True, use GPU\n",
    "    -- verbose     : If set to True and parameter out_step is given, the number\n",
    "                     of calculated batches is reported.\n",
    "    Returns:\n",
    "    -- A numpy array of dimension (num images, dims) that contains the\n",
    "       activations of the given tensor when feeding inception with the\n",
    "       query tensor.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    if len(files) % batch_size != 0:\n",
    "        print(\n",
    "            (\n",
    "                \"Warning: number of images is not a multiple of the \"\n",
    "                \"batch size. Some samples are going to be ignored.\"\n",
    "            )\n",
    "        )\n",
    "    if batch_size > len(files):\n",
    "        print(\n",
    "            (\n",
    "                \"Warning: batch size is bigger than the data size. \"\n",
    "                \"Setting batch size to data size\"\n",
    "            )\n",
    "        )\n",
    "        batch_size = len(files)\n",
    "\n",
    "    n_batches = len(files) // batch_size\n",
    "    n_used_imgs = n_batches * batch_size\n",
    "\n",
    "    pred_arr = np.empty((n_used_imgs, dims))\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        if verbose:\n",
    "            print(\"\\rPropagating batch %d/%d\" % (i + 1, n_batches), end=\"\", flush=True)\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "\n",
    "        images = np.array([imread(str(f)).astype(np.float32) for f in files[start:end]])\n",
    "\n",
    "        images = images[:, :, :, 0:3]\n",
    "        # Reshape to (n_images, 3, height, width)\n",
    "        images = images.transpose((0, 3, 1, 2))\n",
    "        # images = images[0,:,:,:]\n",
    "        images /= 255\n",
    "\n",
    "        batch = torch.from_numpy(images).type(torch.FloatTensor)\n",
    "        if cuda:\n",
    "            batch = batch.cuda()\n",
    "\n",
    "        pred = model(batch)[0]\n",
    "\n",
    "        # If model output is not scalar, apply global spatial average pooling.\n",
    "        # This happens if you choose a dimensionality not equal 2048.\n",
    "\n",
    "        # if pred.shape[2] != 1 or pred.shape[3] != 1:\n",
    "        #    pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
    "\n",
    "        pred_arr = (\n",
    "            pred.cpu()\n",
    "            .data.numpy()\n",
    "            .transpose(0, 2, 3, 1)\n",
    "            .reshape(batch_size * pred.shape[2] * pred.shape[3], -1)\n",
    "        )\n",
    "\n",
    "    if verbose:\n",
    "        print(\"done\")\n",
    "\n",
    "    return pred_arr\n",
    "\n",
    "\n",
    "def calculate_activation_statistics(\n",
    "    files, model, batch_size=1, dims=64, cuda=False, verbose=False\n",
    "):\n",
    "    \"\"\"Calculation of the statistics used by the FID.\n",
    "    Params:\n",
    "    -- files       : List of image files paths\n",
    "    -- model       : Instance of inception model\n",
    "    -- batch_size  : The images numpy array is split into batches with\n",
    "                     batch size batch_size. A reasonable batch size\n",
    "                     depends on the hardware.\n",
    "    -- dims        : Dimensionality of features returned by Inception\n",
    "    -- cuda        : If set to True, use GPU\n",
    "    -- verbose     : If set to True and parameter out_step is given, the\n",
    "                     number of calculated batches is reported.\n",
    "    Returns:\n",
    "    -- mu    : The mean over samples of the activations of the inception model.\n",
    "    -- sigma : The covariance matrix of the activations of the inception model.\n",
    "    \"\"\"\n",
    "    act = get_activations(files, model, batch_size, dims, cuda, verbose)\n",
    "    mu = np.mean(act, axis=0)\n",
    "    sigma = np.cov(act, rowvar=False)\n",
    "    return torch.from_numpy(mu), torch.from_numpy(sigma)\n",
    "\n",
    "\n",
    "def calculate_sifid_given_paths(real_path, path2, batch_size, cuda, dims, suffix):\n",
    "    \"\"\"Calculates the SIFID of two paths\"\"\"\n",
    "\n",
    "    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n",
    "\n",
    "    model = InceptionV3([block_idx])\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    real_path = pathlib.Path(real_path)\n",
    "\n",
    "    path2 = pathlib.Path(path2)\n",
    "    files2 = sorted(list(path2.glob(f\"*.{suffix}\")))\n",
    "\n",
    "    m1, s1 = calculate_activation_statistics(\n",
    "        [real_path], model, batch_size, dims, cuda\n",
    "    )\n",
    "\n",
    "    fid_values = []\n",
    "    Im_ind = []\n",
    "    for f2 in tqdm(files2):\n",
    "        m2, s2 = calculate_activation_statistics(\n",
    "            [f2], model, batch_size, dims, cuda\n",
    "        )\n",
    "        fid_values.append(frechet_distance(m1, s1, m2, s2))\n",
    "        file_name1 = real_path.name\n",
    "        file_name2 = f2.name\n",
    "        Im_ind.append(file_name1[:-4])\n",
    "        Im_ind.append(file_name2[:-4])\n",
    "    return fid_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_name = \"birds.png\"\n",
    "real_path = f\"Input/Images/{input_name}\"  # Path to the real images\n",
    "fake_directory = f\"Output/RandomSamples/{input_name[:-4]}/gen_start_scale=0\"  # Path to generated images\n",
    "gpu = \"0\"  # GPU to use (leave blank for CPU only)\n",
    "images_suffix = input_name[-3:]  # image file suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "\n",
    "sifid_values = calculate_sifid_given_paths(\n",
    "    real_path, fake_directory, 1, gpu != \"\", 64, images_suffix\n",
    ")\n",
    "\n",
    "sifid_values = np.asarray(sifid_values, dtype=np.float32)\n",
    "np.save(f\"Output/RandomSamples/{input_name[:-4]}/gen_start_scale=0/SIFID.npy\", sifid_values)\n",
    "print(\"SIFID: \", sifid_values.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{\"frechet\" if frechet else \"default\"}\\n\\t`niter`: ${niter}$, `Gsteps`: ${Gsteps}$, `scale_factor`: ${scale_factor}$, `mode_pad`: ${mode_pad}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory in [\"TrainedModels\", \"Output\"]:\n",
    "    os.rename(directory, f\"{directory}_{\"frechet\" if frechet else \"default\"}_{niter}_{Gsteps}_{scale_factor}_{mode_pad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Default SinGAN\n",
    "  - `niter`: $1000$, `Gsteps`: $1$, `scale_factor`: $0.75$, `mode_pad`: $0$\n",
    "    - `birds.png`\n",
    "      - Execution time: $28$ min\n",
    "      - SIFID: `2.1939146e-05` \n",
    "  - `niter`: $1000$, `Gsteps`: $1$, `scale_factor`: $0.75$, `mode_pad`: $1$\n",
    "    - `birds.png`\n",
    "      - Execution time: $30$ min\n",
    "      - SIFID: `1.2019249e-05` \n",
    "  - `niter`: $1000$, `Gsteps`: $1$, `scale_factor`: $0.75$, `mode_pad`: $2$\n",
    "    - `birds.png`\n",
    "      - Execution time: $29$ min\n",
    "      - SIFID: `1.227873e-05` \n",
    "  - `niter`: $1000$, `Gsteps`: $1$, `scale_factor`: $0.75$, `mode_pad`: $3$\n",
    "    - `birds.png`\n",
    "      - Execution time: $24$ min\n",
    "      - SIFID: `0.00011935688` \n",
    "  - `niter`: $2000$, `Gsteps`: $1$, `scale_factor`: $0.75$, `mode_pad`: $0$\n",
    "    - `birds.png`\n",
    "      - Execution time: $60$ min\n",
    "      - SIFID: `8.802319e-06` \n",
    "  - `niter`: $2000$, `Gsteps`: $1$, `scale_factor`: $0.75$, `mode_pad`: $2$\n",
    "    - `birds.png`\n",
    "      - Execution time: $62$ min\n",
    "      - SIFID: `1.383028e-05` \n",
    "  - `niter`: $2000$, `Gsteps`: $1$, `scale_factor`: $0.75$, `mode_pad`: $3$\n",
    "    - `birds.png`\n",
    "      - Execution time: $52$ min\n",
    "      - SIFID: `0.00013286904` \n",
    "- SinGAN with Fréchet Induction Distance cost function\n",
    "  - `niter`: $1000$, `Gsteps`: $1$, `scale_factor`: $0.75$, `mode_pad`: $0$\n",
    "    - `birds.png`\n",
    "      - Execution time: $16$ min\n",
    "      - SIFID: `1.1237572e-05` \n",
    "  - `niter`: $2000$, `Gsteps`: $1$, `scale_factor`: $0.75$, `mode_pad`: $0$\n",
    "    - `birds.png`\n",
    "      - Execution time: $31$ min\n",
    "      - SIFID: `1.1789973e-05`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
