{"cells":[{"cell_type":"markdown","metadata":{"id":"TdpPHz2Lp6VD"},"source":["# IMA 206 - Coding autoencoders in Pytorch\n","\n","The lab was originally created by Alasdair Newson (https://sites.google.com/site/alasdairnewson/)\n","\n","The current version is made by Loic Le Folgoc. If you have questions, please contact me at loic dot lefolgoc at telecom-paris dot fr.\n","\n","## Objective:\n","\n","The goal of this TP is to explore autoencoders and variational autoencoders applied to a simple dataset. In this first part, we will look at an autoencoder applied to MNIST. We recall that an autoencoder is a neural network with the following general architecture:\n","\n","\n","![AUTOENCODER](https://drive.google.com/uc?id=11dfNujSHa2-_eThp2aTpL1M_hLaEQX-G)\n","\n","\n","The tensor $z$ in the middle of the network is called a __latent code__, and it belongs to the latent space. It is this latent space which is interesting in autoencoders (for image synthesis, editing, etc).\n","\n","## Your task:\n","You need to add the missing parts in the code (parts between # --- START CODE HERE and # --- END CODE HERE or # FILL IN CODE or ...)"]},{"cell_type":"markdown","metadata":{"id":"gp13aVUQq1WX"},"source":["First of all, let's load some packages:"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"JqNeIJ8Op8Ao"},"outputs":[],"source":["import pdb\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torchvision import datasets, transforms\n","from torchvision.utils import save_image\n","from tqdm import tqdm\n","\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"Hyj5dj_eui9D"},"source":["First, we load the mnist dataset. I find that training on the full training dataset `mnist_trainset` is fast enough even on CPU (5-10 minutes), but should you need it, we create a reduced trainset below.\n","\n","Feel free to train on `mnist_trainset_reduced` instead if you prefer (results might be of poorer quality). To do so, replace the argument `mnist_trainset` in the `torch.utils.data.DataLoader(...)` call creating `mnist_train_loader` in the cell below by `mnist_trainset_reduced` (and same for `mnist_testset` and `mnist_testset_reduced`)."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5791,"status":"ok","timestamp":1714494454110,"user":{"displayName":"Loïc Le Folgoc","userId":"12950297279464732378"},"user_tz":-120},"id":"4YPLKlPrufSk","outputId":"376768cb-21e9-478d-cea6-f5444103a795"},"outputs":[],"source":["batch_size = 64\n","\n","# MNIST Dataset\n","mnist_trainset = datasets.MNIST(\n","    root=\"./mnist_data/\", train=True, transform=transforms.ToTensor(), download=True\n",")\n","mnist_testset = datasets.MNIST(\n","    root=\"./mnist_data/\", train=False, transform=transforms.ToTensor(), download=False\n",")\n","\n","# create data loader with smaller dataset size\n","max_mnist_size = 5000\n","mnist_trainset_reduced = torch.utils.data.random_split(\n","    mnist_trainset, [max_mnist_size, len(mnist_trainset) - max_mnist_size]\n",")[0]\n","mnist_train_loader = torch.utils.data.DataLoader(\n","    mnist_trainset, batch_size=batch_size, shuffle=True, drop_last=False\n",")\n","\n","# download test dataset\n","max_mnist_size = 1000\n","mnist_testset_reduced = torch.utils.data.random_split(\n","    mnist_testset, [max_mnist_size, len(mnist_testset) - max_mnist_size]\n",")[0]\n","mnist_test_loader = torch.utils.data.DataLoader(\n","    mnist_testset, batch_size=batch_size, shuffle=False, drop_last=False\n",")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1714494454110,"user":{"displayName":"Loïc Le Folgoc","userId":"12950297279464732378"},"user_tz":-120},"id":"r7YhlBT2PN9I","outputId":"3d465c07-cd26-4bf1-b758-d68a47db6b4e"},"outputs":[{"data":{"text/plain":["torch.Size([60000, 28, 28])"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["mnist_trainset.data.shape"]},{"cell_type":"markdown","metadata":{"id":"t-bkK4ktwfvC"},"source":["# 1. Vanilla Autoencoder\n","\n","Now, we define our autoencoder model. The autoencoder class `AEModel` is made of an `Encoder` and a `Decoder`, which we create first. We will reuse the `Encoder` and `Decoder` classes when building our variational autoencoder model, and wrap them in a `VAEModel` instead."]},{"cell_type":"markdown","metadata":{"id":"6jLa2-jQwxSI"},"source":["We will use the following convolutional architectures :\n","\n","__Encoder__ :\n","- Conv layer, 32 filters, 4x4 kernel, stride=2, padding=1; + ReLU\n","- Conv layer, 32 filters, 4x4 kernel, stride=2, padding=0; + ReLU\n","- Conv layer, 32 filters, 4x4 kernel, stride=2, padding=0; + ReLU\n","- Flatten\n","- Dense layer with 64 output neurons; + ReLU\n","- Dense layer with `self.latent_dim*self.multiplier` output neurons.\n","\n","For the autoencoder, `self.multiplier=1` as the encoder outputs a `self.latent_dim`-dimensional latent code. For the variational autoencoder, we will set `self.multiplier=2` as the encoder will output `self.latent_dim`-dimensional mean and log-variance parameters of the Gaussian distribution $q_\\phi(z|x)$.\n","\n","__Decoder__ (the decoder of the AE and VAE are the same, they always outputs an image/probability map, given a code $z$ as input):\n","- Dense layer with 64 output neurons; + ReLU\n","- Dense layer with ??? output neurons; + ReLU\n","- Reshape, to a `(C, H, W)` tensor with `C=32`, `H=???`, `W=???`.\n","- Conv transpose layer, 32 filters, 4x4 kernel, stride=2, padding=0; +ReLU\n","- Conv transpose layer, 32 filters, 4x4 kernel, stride=2, padding=0; +ReLU\n","- Conv transpose layer, 1 filter, 4x4 kernel, stride=2, padding=1; +Sigmoid\n","\n","The number of output neurons of the second dense layer is exactly the number of input neurons in the first dense layer of the encoder (i.e., the number of values in the feature maps of the conv layer immediately before it).\n","\n","For the reshape operations, use the ```A.view(dim_1,dim_2,...)``` function, where ```A``` is your tensor."]},{"cell_type":"markdown","metadata":{"id":"eNH7ScylwKa-"},"source":["__Hint for computing the number of neurons that are not given to you__: This [great resource](https://madebyollin.github.io/convnet-calculator/) lets you compute the size of the output tensor following any convolution layer depending on the input tensor shape and conv parameters."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"0-IyPUQkIR-U"},"outputs":[],"source":["class Encoder(torch.nn.Module):\n","    def __init__(self, latent_dim=10, multiplier=1):\n","        super(Encoder, self).__init__()\n","\n","        # Layer parameters\n","        self.latent_dim = latent_dim\n","        self.multiplier = multiplier\n","\n","        # Shape at the end of conv3\n","        self.reshape = (32, 2, 2)\n","\n","        # Convolutional layers\n","        self.conv1 = nn.Conv2d(\n","            in_channels=1, out_channels=32, kernel_size=4, stride=2, padding=1\n","        )\n","        self.conv2 = nn.Conv2d(\n","            in_channels=32, out_channels=32, kernel_size=4, stride=2, padding=0\n","        )\n","        self.conv3 = nn.Conv2d(\n","            in_channels=32, out_channels=32, kernel_size=4, stride=2, padding=0\n","        )\n","\n","        # Fully connected layers\n","        self.lin1 = nn.Linear(in_features=np.prod(self.reshape), out_features=64)\n","        self.lin2 = nn.Linear(in_features=64, out_features=latent_dim * multiplier)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Pass the input image mini-batch through conv, linear layers and\n","        non-linearities to output a (B, D, 2) tensor where B is the mini-batch\n","        size and D the latent dimension.\n","        \"\"\"\n","        batch_size = x.size(0)\n","\n","        # Convolutional layers with ReLu activations\n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        x = self.conv3(x)\n","        x = F.relu(x)\n","\n","        # Flatten\n","        x = x.view(batch_size, -1)\n","\n","        # Fully connected layer with ReLu activation\n","        x = self.lin1(x)\n","        x = F.relu(x)\n","\n","        # Fully connected layer for code z, or mean and log-variance\n","        output = self.lin2(x)\n","\n","        # The shape of the output tensor should be (B, D) if multiplier=1,\n","        # where B is the batch size, and D the latent dimension.\n","        # Otherwise it should be (B, D, multiplier)\n","        if self.multiplier != 1:\n","            output = output.view(batch_size, self.latent_dim, self.multiplier)\n","\n","        return output"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"U64AqYvpYpr6"},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, latent_dim=10):\n","        super(Decoder, self).__init__()\n","\n","        # Shape required to start transpose convs (copy paste from the encoder)\n","        self.reshape = (32, 2, 2)\n","\n","        # Fully connected layers\n","        self.lin1 = nn.Linear(in_features=latent_dim, out_features=64)\n","        self.lin2 = nn.Linear(in_features=64, out_features=np.prod(self.reshape))\n","\n","        # Convolutional layers\n","        self.convT1 = nn.ConvTranspose2d(\n","            in_channels=32, out_channels=32, kernel_size=4, stride=2, padding=0\n","        )\n","        self.convT2 = nn.ConvTranspose2d(\n","            in_channels=32, out_channels=32, kernel_size=4, stride=2, padding=0\n","        )\n","        self.convT3 = nn.ConvTranspose2d(\n","            in_channels=32, out_channels=1, kernel_size=4, stride=2, padding=1\n","        )\n","\n","    def forward(self, z):\n","        batch_size = z.size(0)\n","\n","        # Fully connected layers with ReLu activations\n","        x = self.lin1(z)\n","        x = F.relu(x)\n","        x = self.lin2(x)\n","        x = F.relu(x)\n","\n","        # Reshape\n","        x = x.view(batch_size, *self.reshape)\n","\n","        # Convolutional layers with ReLu activations\n","        x = self.convT1(x)\n","        x = F.relu(x)\n","        x = self.convT2(x)\n","        x = F.relu(x)\n","\n","        # Final conv layer with sigmoid activation\n","        x = self.convT3(x)\n","        output = F.sigmoid(x)\n","\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"JmGyzVp4HVRJ"},"source":["The autoencoder model itself is basically a wrapper around an `Encoder` and a `Decoder`. In the forward pass, the input images contained in the tensor `x` are passed through the `Encoder` to obtain the latent codes `z` then these codes are fed to the `Decoder` to produce the reconstructions `y`."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"T9MYicH9Zf3o"},"outputs":[],"source":["class AEModel(nn.Module):\n","    def __init__(self, latent_dim):\n","        \"\"\"\n","        Class which defines model and forward pass.\n","\n","        Parameters\n","        ----------\n","        latent_dim : int\n","            Dimensionality of latent code.\n","        \"\"\"\n","        super(AEModel, self).__init__()\n","\n","        self.latent_dim = latent_dim\n","        self.encoder = Encoder(latent_dim)\n","        self.decoder = Decoder(latent_dim)\n","\n","    def forward(self, x, mode=\"sample\"):\n","        \"\"\"\n","        Forward pass of model, used for training or reconstruction.\n","\n","        Parameters\n","        ----------\n","        x : torch.Tensor\n","            Batch of data. Shape (batch_size, n_chan, height, width)\n","\n","        Outputs a dictionary containing:\n","          codes - the latent codes corresponding to the input images\n","          reconstructions - the images reconstructed by the autoencoder\n","        \"\"\"\n","\n","        # z is the output of the encoder\n","        z = self.encoder(x)\n","\n","        # Decode the samples to image space\n","        y = self.decoder(z)\n","\n","        # Return everything:\n","        return {\"reconstructions\": y, \"codes\": z}"]},{"cell_type":"markdown","metadata":{"id":"VeWePstgIVBw"},"source":["Next, we carefully create the reconstruction loss. It will be reused for the VAE loss later on."]},{"cell_type":"markdown","metadata":{"id":"3FIb4BDrInYy"},"source":["The reconstruction loss translates a pixel-wise Bernoulli probabilistic model into a loss (`F.binary_cross_entropy`). It takes input images `data` and reconstructed probability maps `reconstructions` and computes the binary cross-entropy, from the two images."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"CWrQbhOnbv7y"},"outputs":[],"source":["def reconstruction_loss(reconstructions, data):\n","    \"\"\"\n","    Calculates the reconstruction loss for a batch of data. I.e. negative\n","    log likelihood.\n","\n","    Parameters\n","    ----------\n","    data : torch.Tensor\n","        Input data (e.g. batch of images). Shape : (batch_size, 1,\n","        height, width).\n","\n","    reconstructions : torch.Tensor\n","        Reconstructed data. Shape : (batch_size, 1, height, width).\n","\n","    Returns\n","    -------\n","    loss : torch.Tensor\n","        Binary cross entropy, AVERAGED over images in the batch but SUMMED over\n","        pixel and channel.\n","    \"\"\"\n","    batch_size, n_chan, height, width = reconstructions.size()\n","\n","    # The pixel-wise loss is the binary cross-entropy, computed from\n","    # reconstructions and data. It is summed over pixels and averaged across\n","    # samples in the batch.\n","    loss = F.binary_cross_entropy(reconstructions, data, reduction=\"sum\")\n","    loss /= batch_size\n","\n","    return loss"]},{"cell_type":"markdown","metadata":{"id":"iqibefFRJHy_"},"source":["### Training the vanilla autoencoder"]},{"cell_type":"markdown","metadata":{"id":"RQbHTXMLJODw"},"source":["The training proceeds as usual. We instantiate a model, move it to the correct device, create an optimizer and write the training loop."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"NyZcTZP3a_kc"},"outputs":[],"source":["# Parameters\n","latent_dim = 10\n","\n","learning_rate = 1e-3\n","n_epoch = 5  # if running on GPU you can use more epochs (10 or more)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1714494454476,"user":{"displayName":"Loïc Le Folgoc","userId":"12950297279464732378"},"user_tz":-120},"id":"ft_3txj0bZjO","outputId":"13273cfd-eaeb-4d63-9d05-5b66ae5349a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n"]}],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"oV40vRMQRoG1"},"outputs":[],"source":["# Model\n","ae_model = AEModel(latent_dim)\n","ae_model = ae_model.to(device)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"sbnKjRTDaynz"},"outputs":[],"source":["# Use the AdamW optimizer, set the correct learning rate and weight_decay to 1e-4\n","optimizer = optim.AdamW(ae_model.parameters(), lr=learning_rate, weight_decay=1e-4)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"S2-phJydcICh"},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 0:   0%|          | 0/938 [00:00<?, ?batch/s]c:\\ProgramData\\Miniconda3\\envs\\ml-torch\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n","  return F.conv2d(input, weight, bias, self.stride,\n","c:\\ProgramData\\Miniconda3\\envs\\ml-torch\\Lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n","  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n","Epoch 0: 100%|██████████| 938/938 [00:19<00:00, 47.02batch/s, loss=131]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 0: Train Loss: 178.7752\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1: 100%|██████████| 938/938 [00:20<00:00, 46.62batch/s, loss=103] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss: 113.6891\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2: 100%|██████████| 938/938 [00:21<00:00, 44.48batch/s, loss=97.5]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss: 99.6826\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3: 100%|██████████| 938/938 [00:17<00:00, 53.64batch/s, loss=88.2]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss: 92.3108\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4: 100%|██████████| 938/938 [00:14<00:00, 65.10batch/s, loss=92.5]"]},{"name":"stdout","output_type":"stream","text":["Epoch 4: Train Loss: 89.2893\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["ae_model.train()\n","\n","for epoch in range(0, n_epoch):\n","    train_loss = 0.0\n","\n","    with tqdm(mnist_train_loader, unit=\"batch\") as tepoch:\n","        for data, labels in tepoch:\n","            tepoch.set_description(f\"Epoch {epoch}\")\n","\n","            # Put data on correct device, GPU or CPU\n","            data = data.to(device)\n","\n","            # Pass the input data through the model\n","            predict = ae_model(data)\n","            reconstructions = predict[\"reconstructions\"]\n","\n","            # Compute the AE loss\n","            loss = reconstruction_loss(reconstructions, data)\n","\n","            # Backpropagate\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Aggregate the training loss for display at the end of the epoch\n","            train_loss += loss.item()\n","\n","            # tqdm bar displays the loss\n","            tepoch.set_postfix(loss=loss.item())\n","\n","    print(\n","        \"Epoch {}: Train Loss: {:.4f}\".format(\n","            epoch, train_loss / len(mnist_train_loader)\n","        )\n","    )"]},{"cell_type":"markdown","metadata":{"id":"iHscBN4KJ2Sq"},"source":["### Testing the vanilla autoencoder"]},{"cell_type":"markdown","metadata":{"id":"w3EbmswSzJdK"},"source":["We define functions for qualitative testing of the autoencoder model. We will reuse them throughout the lab."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"T8jXjdRyzMy2"},"outputs":[],"source":["def display_images(imgs):\n","    \"\"\"\n","    Display a batch of images (typically synthetic/generated images)\n","    \"\"\"\n","    r = 1\n","    c = imgs.shape[0]\n","    fig, axs = plt.subplots(r, c)\n","    for j in range(c):\n","        # black and white images\n","        axs[j].imshow(imgs[j, 0, :, :].detach().cpu().numpy(), cmap=\"gray\")\n","        axs[j].axis(\"off\")\n","    plt.show()\n","\n","\n","def display_ae_images(ae_model, test_imgs):\n","    \"\"\"\n","    Display a batch of input images along with their reconstructions by a given model\n","      First row: input images\n","      Second row: reconstructed images\n","    \"\"\"\n","    n_images = 5\n","    idx = np.random.randint(0, test_imgs.shape[0], n_images)\n","    test_imgs = test_imgs[idx, :, :, :]\n","\n","    # get output images\n","    output_imgs = ae_model(test_imgs.to(ae_model.encoder.conv1.weight.device))[\n","        \"reconstructions\"\n","    ]\n","    output_imgs = output_imgs.detach().cpu().numpy()\n","\n","    r = 2\n","    c = n_images\n","    fig, axs = plt.subplots(r, c)\n","    for j in range(c):\n","        axs[0, j].imshow(test_imgs[j, 0, :, :], cmap=\"gray\")\n","        axs[0, j].axis(\"off\")\n","        axs[1, j].imshow(output_imgs[j, 0, :, :], cmap=\"gray\")\n","        axs[1, j].axis(\"off\")\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"TQiEplCiKnlT"},"source":["Let's see how well the autoencoder reconstructs images from the training set:"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"9pbXch29d68D"},"outputs":[{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAgQAAAEzCAYAAABOlRseAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAfz0lEQVR4nO3de5RVdRnG8RcQEVEGdCbuyMRFSBRCLgKhBEp4QUAuy2tJmaiYZuLSllqhK5OwtCxFRTNqrUyXgJpCDgQYoBEXQUWQeyAwjoBKXiCR/mj58uzT2c4+M+e6z/fz1wOeYfacffb09v5udQ4dOnTIAABAUaub6wsAAAC5R0EAAAAoCAAAAAUBAAAwCgIAAGAUBAAAwCgIAACAURAAAACjIAAAAGZ2RNQX1qlTJ5PXUbTSsVEk9yYzantvuC+ZwTOTv3hm8lPU+0KHAAAAUBAAAAAKAgAAYBQEAADAKAgAAIBREAAAAKMgAAAARkEAAACMggAAABgFAQAAsBS2LgY+V69ePc+TJ0/2fOONN3qeOnWq50mTJnmurKz0nI4taIFCM2XKFM/Nmzf3fNlll+XicpCCI4880nODBg2Svmb//v2eDxw4kPFrSic6BAAAgIIAAADEaMjgoosu8ly37uE654wzzvB8xRVXJP1aPWFL29g33HCD59mzZwe+5q233qr5xRa4Zs2aeT7rrLM863s3fvz4pPmhhx7yrEMJu3btSvt14ovxzOSGvl8XXnih571793q+7rrrsnpNCGrYsKHnW265xfPAgQM9DxgwwLM+Dy+88ILnMWPGeP7oo4/SfZlpR4cAAABQEAAAALM6hyJO9daWSC6VlJR41jbnjh07POtMUJ3xGdayady4sWedQa/WrFkT+PPgwYM9v/POO9Vddqh0zLTP5b1p3bq15z/96U+ee/fu7bl+/fpJv/bgwYOedVXCrbfe6vmDDz5Iy3XWRG3vDc8Mz0wyuppg2rRpnvft2+f5G9/4hufly5dn58LSoBCeGV0d0K9fP896X9q1a+dZhwnUxo0bk/69vgc//OEPPT/99NOpXmraRL0vdAgAAAAFAQAAyOMhgyFDhnh+8cUXPWubU2e7a/tmzpw5nl9++WXP2tJWd999t+ebbrop0vXpTNJhw4ZF+ppkCr39Gebss8/2rC033bxIW836PlRUVHgOm4WdDYXQ/lQ8M9HlyzMzY8YMz8OHD/esz8DQoUNT+jevvPJKz3PnzvW8adOmmlxiSvLxmUncQEg3hrr22muTfs26des8//KXv/Ssv4NmzZrl+dNPP63tZWYUQwYAACAyCgIAAEBBAAAA8mynwpEjR3q++eabPetykCVLlni+5557PO/Zs8dzqjuiTZ8+3XPU8VB8Md2lTrOOaT766KOedfmi7n748MMPe9Zdv/A/PDOFTe/NiBEjPNdmLL1jx46eX3nllRr/O4Wsffv2nr///e8H/tuECRM867Jm/Rz/+c9/TvqauKNDAAAAKAgAAECOhwy03WlmNnPmzKQZhU13/TriiMMfuXxZ+lVIeGbiS5eG6QFTZWVlnquqqlL6d4rJ0Ucf7fmll17y3KJFi8DrdImtHly0evXqDF5dYaBDAAAAKAgAAEAOhgy0/fXNb34z8N+y2fLUHdvuv//+al+/bdu2wJ91xz38P92p8N577/XcqVMnzzpkoG1Ofa+/+93vZuoSCwbPTPHRw6a0FR6FPm/vvfdeui4p7+nOpzpMkPg5vPjiiz1n8/1p2rSp5+7du3ueP39+1q6hOnQIAAAABQEAAMjBkEGrVq08/+Uvf8nq99Yz3J944gnPp59+etLXb9myxfOoUaMC/y3VjVziSjcU0oNwevfu7bl+/fpJv1YPBLn00ks962YqxdTyDMMzU9xGjx7t+Re/+EW1r9+5c2cmLydvXXXVVUn//r777gv8OVe/UyZPnuxZf9/ps7Rs2bKsXlMiOgQAAICCAAAA5GDI4NVXX02a00lnm2o7prS0NOnfKz0z/LzzzvOs52PjsGnTpnnu379/0tesWrXKs24Yctddd3murKzMwNXFA89MfGl7X1fdsGlX6vQMFLV3794sX0lya9as8XzUUUd5/trXvuaZIQMAAJBzFAQAACC/jj9OFz3eUjfpCLNjxw7POqOXlmf1GjVqVO1r7r77bs96rCjyB89MbsybN89z2BkEulojyiqDH//4x56feuopz9qyjiMdmjzzzDM9Jx7P/fjjj2frkgL0qGs1ZcoUz5988kngv02dOjWj15SIDgEAAKAgAAAABThkcO6553rWmZq/+93vPDds2DClf1M3qtC2E6q3YMECzzpbVumGNtddd53nuXPnev75z3/u+cMPP0zjFYJnJn/pZ103dSovL/es9ywKHcbT48bj7o477vA8ceJEz4nHH/fo0cPzihUrMnpNukrn97//vWddRaL36Prrrw98vQ757N69OxOXGECHAAAAUBAAAIA8GzLQPe+1jXLJJZd4/vWvf+25QYMGafm+deserov0++pe+0ju0Ucf9Xzcccd51iNGS0pKPPft29dzv379PP/gBz/wrMMHd955Z/ouNoZ4Zgrbvn37PK9fv96zDhnosbmawzbcWblyZbWviaP9+/d7XrhwoefEDbV0lUGvXr2Sfn1t6Pkfl19+uecTTzzRc9iKEn2NWfCsGIYMAABAVlAQAAAAq3MorHeR+MIM7K09bNiwwJ/HjRvnefjw4TX+d3WzD525qzOptaWtbrvtNs8/+9nPanwNUUV8+79QPu573qFDB8833HCD57Fjx3rWGbj6PmgbVY8u1Q1XsqG294ZnJjPi+szo5/snP/mJZ/15u3fv7nn16tXZuKyU5MszM2nSJM+333576OsqKio86wZquomQrlLYvn27Zx0e0993P/rRjzzrMce7du3yrEOt+juxY8eOgevTlUDf+c53Qn+O6kS9L3QIAAAABQEAAMjSKoM2bdp41hmeukGEWXB2Zhg9JvfgwYOev/3tb3vWI2Krqqo8d+7c2XNY+xPpsWHDBs+6T/7999/vWTfhGD9+vOdjjz3W8/e+9z3PM2fO9JypY4DzBc9M8dFjpLXFq1nPNcjHIYN8oe3/t99+O/S/6ZHJmnXTNF0ZNX/+fM967sCYMWM86/3SYYLevXt71qGHLl26eE4cMvj617/uuUmTJp51Y7B0okMAAAAoCAAAQJaGDM4//3zPAwcOjPQ1Ortyz549nnWjGp2NHsVvf/vblF6P9Fu7dq1nHQ7QGbs6xKBtsubNm2f24vIIz0zxiXI8sW5MhHAff/yx54cffjjw35YuXepZhy1HjhzpWY9PVuedd17Sv9+6davnadOmeX7ppZc86zCB0hUHF1xwQeC/tWvXzvO3vvUtz7/61a+S/lu1RYcAAABQEAAAgCxtTKR7nXft2jXS12hrWWdzRqH7rOssaW3N6IxPnd2rR/jq7OxMiesmK6nScxD0ON1WrVp5PuecczzPmTMn49eUy01WeGbCFcMz89lnn3nWn1ff35YtW2b1mqLIl42JauKUU07xHGX1jtIhg23btqX0tT179vSsG4SZBVdcPfvss55Hjx7tOcr5IWxMBAAAIqMgAAAA2VlloC2NbGwoo/tH68zrMNpezUbLM060XaybDj344IOedRONGTNmeNbPRdu2bT3rMbA6ZKB7eWdjyCCXeGaK2x//+EfPem+iuPrqqz0vXrzYMxsZfbFcvT/Lli3zrCsJzMymT5/uWVceXXPNNZ71ePPaokMAAAAoCAAAQJaGDLLhtNNO86xHwqoDBw541jbNzTffnLkLizmdBdutWzfPU6dOTfr6zZs3e9aZr8cff7znkpKSpF/brFmzGl8n/h/PTP5asWKF50suucRzWVmZ5xEjRnieNWuW5/Lycs86ZID8p/fRLDgcd8wxx3jWo7IZMgAAAGlFQQAAACgIAABAjucQTJw4MfDnE044wfNPf/pTz3qm9F133eVZx6/Hjx/vuV69ekm/3z333OP59ttvr8EVI9Hf/vY3z7feeqvnQYMGeR48eLBnHd/UXcnCdtKaPXu2Zx03K1Y8M8Vh5cqVnnXXQt1RMmwOAeJDlx2WlpZm/PvRIQAAABQEAAAgg4cbjR071nPYEjRdRmEWbFvque3aMtNDJ/SadOc6ff3ChQs933fffZ6jHAiRDXE9qKV+/fqe+/fv7/mss87yfNNNN3meMmWK5z179njWc7+zfc+yfVALz0w0cX1mwkyaNMnzbbfd5vkPf/iD58svv9zzRRdd5HnRokWeUz10pyYK+XCjOONwIwAAEBkFAQAAyNyQQbt27Tz37ds30tcMHz7c85gxY5K+ZsKECZ7ff/99z08++aTngwcPRr3MnCu29mchyXb7k2cmGp6Z/MWQQX5iyAAAAERGQQAAADI3ZIBoaH/mL9qf+YlnJn/xzOQnhgwAAEBkFAQAAICCAAAAUBAAAACjIAAAAEZBAAAAjIIAAAAYBQEAALAUNiYCAADxRYcAAABQEAAAAAoCAABgFAQAAMAoCAAAgFEQAAAAoyAAAABGQQAAAIyCAAAAGAUBAAAwCgIAAGAUBAAAwCgIAACAURAAAACjIAAAAEZBAAAAjIIAAAAYBQEAADAKAgAAYBQEAADAKAgAAIBREAAAAKMgAAAARkEAAACMggAAABgFAQAAMAoCAABgFAQAAMAoCAAAgFEQAAAAoyAAAABGQQAAAIyCAAAAGAUBAAAwCgIAAGAUBAAAwCgIAACAURAAAACjIAAAAEZBAAAAjIIAAAAYBQEAADAKAgAAYBQEAADAKAgAAIBREAAAAKMgAAAARkEAAACMggAAABgFAQAAMAoCAABgFAQAAMAoCAAAgFEQAAAAoyAAAABGQQAAAIyCAAAAGAUBAAAwCgIAAGAUBAAAwCgIAACAURAAAACjIAAAAEZBAAAAjIIAAAAYBQEAADAKAgAAYBQEAADAKAgAAIBREAAAAKMgAAAARkEAAACMggAAABgFAQAAMAoCAABgFAQAAMAoCAAAgFEQAAAAoyAAAABGQQAAAIyCAAAAGAUBAAAwCgIAAGAUBAAAwCgIAACAURAAAACjIAAAAEZBAAAAjIIAAAAYBQEAADAKAgAAYBQEAADAKAgAAIBREAAAAKMgAAAARkEAAACMggAAABgFAQAAMAoCAABgFAQAAMAoCAAAgFEQAAAAoyAAAABGQQAAAIyCAAAAGAUBAAAwCgIAAGAUBAAAwCgIAACAURAAAACjIAAAAEZBAAAAjIIAAAAYBQEAADAKAgAAYGZHRH1hnTp1MnkdRevQoUO1/je4N5lR23vDfckMnpn8xTOTn6LeFzoEAACAggAAAFAQAAAAoyAAAABGQQAAACyFVQYAgCCdFZ+O1Q/IvFRXMqR6X6P8+/n6WaFDAAAAKAgAAABDBgCQFomt4rC2sL4ubMghX1vKhaJevXqeE+9L2PuvWb/+P//5j+ew+3LEEYf/p7Ru3cP/P/vgwYOeP/3006R/n0/oEAAAAAoCAABQ4EMG2uLRNs2RRx7p+aijjvLcuHFjz9qy0TbQ3r17PX/yySdJX5Psz3EQ1j4zC7bQ9P098cQTPfft29dznz59PDdq1Mizvu//+Mc/PC9evNjz6tWrPX/88cee4/ieZxvPTHpF3iNe3uuysjLP3bp18/zBBx94Xrduned9+/Z5jtJqjuP7HCbs8/xFQwbHHHOM59LS0qRfU79+fc/a6t+/f79n/d2kr2/SpInnPXv2eK6qqkr6tfmEDgEAAKAgAAAAZnUORewvZftYSm3/NGzY0LO2eHr06OH5sssu83zKKad4btq0qWdt6xw4cMCztjxfe+01zxUVFZ5nz54duL7t27d71pZSqvL1KFdtn5kF38dzzz3X86WXXur5pJNO8nz88cd71na0vlebN2/2rC3SyZMne9ZhBZ3tmw2FdpQrz0x02bg3ej/0eRg1apTnMWPGeF6xYoXnWbNmeX7zzTc9f/TRR571GdWhBH1Osj2bPdvPjL7HmnXWvw4RmJl16NDBc/v27T3r0Ix+1nUoR9//Bg0aeC4pKfGsQ0I6xKDDojt37kz682QKxx8DAIDIKAgAAEB+rTLQlk+LFi08X3DBBZ5HjhzpuXPnzp6/9KUvedZW2meffeZZ25TaVjv66KM960z55s2bJ329mdlzzz3n+Z133vEc1xm+2nYrLy/3rDNqP/zwQ8/a+tO/1/dKW27aZtP7unLlSs/ZHjIoBDwzhUHfXx22OeGEEzy/9957nnW1hs5I1yEAvfdhs+31Xsb9fdafT99vXQllZnb66ad71s+xvv86hKl/r1nvRa9evTx36tTJs9679evXe66srPSs9yjX6BAAAAAKAgAAkOMhg8SZ7G3atPF84403ej7//PM9a+ta25nbtm3zrBtA6MxOncWrLbbTTjvNs86UP/bYYz0PGDAgcK0LFizw/O6773rO1z2qU5XYXtSVAjoLXVcKbNmyxfOiRYs8b9y4MenXjh8/3vOpp57qWdvdOlNdZ/jGvf0ZhmemcOhnVD+7Ogymqw82bNjg+d///rfnKO+PDr/pDPm4Pyf68+nn87jjjvM8fPjwwNf079/f88svv+z56aef9qy/18KGbPT76T3t2LFj0uvQHDask2t0CAAAAAUBAACgIAAAAJaDOQQ6dqI7qJmZXX311Z513EfHJXXXrhkzZnh+9tlnPeuSJh2707Ea3YFt2bJlnq+66irPZ599tufu3bsHrrVnz56et27d6jmu46F64I3u3KX3Y+7cuZ71HoQtQdQli82aNfO8e/duzzo2Wqx4ZgqTjm/rjnW6/EzngOg8Dp1DELacTu+HzvGJ+7wBpT+r7s45YsQIz2eccUbga3Spoc6XWbt2rWdd9hmF7jy4dOlSzzpP6pxzzvH8+uuve9bfd7meT0CHAAAAUBAAAIAcDBnoEijdQc3MbNCgQZ7ff/99z88884zne++91/Pbb7/tWZfaqLBlKdqa0fZQq1atPOuSoMRd1/T6wr5HIbfuEg8Z0T//61//8qznfeuSNm1z63DAwIEDPevSNW3jactt165dngv5/awNnpnCp+/1q6++6nns2LGezzzzTM96qFfYEJq2yHUYL9dt52zS3zP6ORw3bpxn3cHTzGzJkiWedXm0DuukSj+3+jtLl/bqAWJDhgzxrAdZ6XBd4r+bDXQIAAAABQEAAMjSkIG2BLXNpdksOHNZd6jTmaB6uESqB3foa/S87MGDB3vWg1q0Vavf18xsx44dKX3vQpP4M2mrWXev09nsOjTQunVrz3rwh87+1ZayzoR/8MEHPesuYcWEZyZe9OfVz722vPWwqS9/+cue9aAd/VrNtWl311bi7pnZpJ9JncXfpUsXz/q7xczsiSee8Bw2jJUqvY+6+kOH1nQlyMknn+x5/vz5nhOH8XQFTjaGgugQAAAACgIAAJCDVQbaAtYWp5lZRUWF5+3bt3tOdZMIpa1XnaGrG6jceeednvUAir1793p+5JFHAv+uHtgTxzPHE9tTuppA/1u3bt0864z3Pn36eNZWqJ7/rptz3HLLLZ43bdrkOS7vZ23wzMSLbjqkh+uMGjXK8xVXXOFZhw/mzZvnWZ+fXK4syOX31iFLHTLQjdR0wyEzs1deecWzroxKlQ6V6FCZrrTRzddatmyZ9PV6PTrUZxYcdtNhCR1KSOfzQ4cAAABQEAAAgCwNGWhLQ/ey15nrZsEZ1Nq21FaJtoLCWihKWzbDhg3zfO2113pu2rSpZ23RPPDAA5518wiz4AYSxdDy1LZg2D7ruvFG7969PWtbT1VWVnresmVL0u9VrHhm4kvf9+XLl3vW97Ft27aehw4d6lnvzYYNGzzrbPlsy/rmOfIZbt++veeuXbt61vdD32Oz4CZOUb6HPleaGzVq5Ll58+aey8vLPetwT1lZmed27dp5vvLKKz2/++67gevQTax0JYleXzqH3+gQAAAACgIAAJCDVQba0kicCa0bN2jWWczaFtK/19aMHimqrRxtaWsbVWe1T5s2zfPjjz/uObElV8xtbZ2Zq7PKGzdu7Flnp+t7rfdcz0Qo5vezOjwz8aJtZ72fupJH71PYPdP7XUz0/dMhFB2S0t9Fek5K4p91OE7pUJw+f2EbEOnqKX1+9Pr0a3XYonPnzp779esXuA5dOaRHlOumVHp9DBkAAIBaoyAAAAC5HTLQFkrin/UoS21b6vGv+hrdR1/bNIlHsH5OZ2s/9thjnqdPn+5ZW3jFPCs6kbbstI2sLWFtiel+47pXd6dOnTzrBke6OUdtNg6JC56ZeNH3RTed0iOPdZMiHTLQYbZiHTLQDYH0d44OGWg7X3/PmAU/9zqspZ973ThIhz91uEE3INKzCfR+6e+v9evXJ/15lA4fmJmdeuqpnletWuVZh2r37dvnOexI86joEAAAAAoCAACQgyEDbTdrW8Ys2NrRmdHa/tTZo9ouCss6i1db12+88YbnJUuWeA7bdAeHaTtOs7axdHasZn1PtXV3/fXXJ/1eem/CWtlxxzMTXzq0tm3bNs/6XOmMd21f64Zf+hmJ+z3Qn0/fM93ER2f66yZAZsFN03QzKJ3Fr0OhugnXSSedVO330Hu3detWz/p7UF+jwwQ7d+4MXOuuXbs86/CD5nSu3qFDAAAAKAgAAEAGhwy0haW05aUtF7Ng20Vbo9oq1j3v9Xu89tprSb+fbhih+16XlpZ61lmh2o4qpjZcKrSdpq2vt956y7Pus757927P2v7U40q/+tWvev7Nb37jWWezT5061bPOzo4Lnpnio8MtK1asSPr3+ozpEI4+h/r3cRxa08+Vtsh173/9naPDl7oawCw4BKBHs+smRSUlJZ67d+/uWY8A102idJhNhy4WLlzouaqqyrOea6CrJnSIIPGa9JnT8zD0jIParsqiQwAAACgIAABAmocMwo6J1NZW69atPSfO/gxrn+mGDLoRi+6nrm0ybQn179/fsx4zqW0k3fxBWzxxbL3VlL7v+r5s3rw56Wu0jaWbqejnQtt9EyZM8Kwz5ydOnOh548aNnp9//nnPYcf4FgKemeKmwyp6D7R1rEM12qbW+7R27VrPulFNOo/GzSW9dv2ZdGOhZ555xrPO3NeVAWbB902fDf09pStzdFVCjx49POv7rMMEf//73z3rKgO9Vh3e0yGhxGFDPWI57HdFOjdvo0MAAAAoCAAAQAZXGWgbpGXLlp611ZjYwtJZokr3R9c2s+5drXSDCZ19O2TIEM/altase1XHcSZ7TYXN8tWhAZ0Rq/dJN1/R18+ZM8dz27ZtPV9zzTWedb/2vn37eq6oqPAcl/vEM1N89FnS1SAqbPWJvu/6nCSed/G5Qh5aU/oM6M+kw5d//etfPevwm1nwvRo4cKBnbb3r51iHD/RZevHFFz0/99xznhNXCnxOhwn0952uHNLhocTvrUOm+rymEx0CAABAQQAAADK4ykA3U+nVq5dn3ehk+fLlga/XNqe2mbUVrS3nsFmzutGD/pv67+hr9MjIsHZbsQvbL1s39igvL/esbefKykrP2pbTe6wzc4cOHer5K1/5imedLaz7uBdym5pnprjpc6XHS+se+/oZ0WGCfv36eV60aJHnpUuXei7kZyMK/TzrZ3X27NmedQWGmVmXLl086yZcmt98803PuiGQ3hc9z0M3QtLnWO+dDtetWbPGc8+ePT3rKhKz4HHl+szpiqKwIaWa4EkGAAAUBAAAIM1DBtpS1FnSffr08dymTRvPenSlWbC1rO2fsJak/r1m/R4DBgzwfPLJJyd9vR5LqW0ZJKfnEQwaNMizHrm7bt06z9pO06ztz5EjR3rWGew6NKBnIsTlPvHM4HN6L1evXu1Z99LXjWo6dOjgWYfWdMggnUfj5jsdjtRZ+Ikz8v/5z38m/Xod5tTnUjeM0o23dDhGnw1dGaC/47S1r8N7uupBj4Q3C/7O06EL/b2YznMs6BAAAAAKAgAAkIYhA22D6MYL2vrQWc56rKTOJjcLti11NrW2fHSWp34/3bt63LhxnnVGqba6X3/9dc9z5871zFGu1dN2lc6G1w10NOtQgn4WtBWnmw5pa0z3C9c2qh43Wmh4ZpCMtvcXLFjgWYdwdEa6Dhl07drVc9je+MV0P77oZw3boCns76P8rtG2v54vovdUhxj0+dZzEHSIziz47OrGVfp7MZ33lQ4BAACgIAAAABQEAADA0rzsUMdg9Oxn3aFJl2Ho2dJmwbHL0aNHe9ZlTTpWo2OrmnXJiC5F0SVb8+fP96xnVut4UTGNuVVH3wtdHrV48WLPem54x44dPesOgzqmplmX7ejyuXnz5nl+/vnnPdd2eU2+4JnB5/SzsGrVqqRZx5h1uZk+SzqHALUXZemmPgM6P0CXF+p8Dj1wSXeo1GfPLLiEUZ9jfV06l5bSIQAAABQEAAAgDUMG2iLUZRV6QIq2vFq0aOFZl6OZmZWVlXnWdqbStpq2YLSFomfE61IsbTnrNVVVVXnWVjTtz+S0JTZz5kzPuqvWxRdf7FmHErTNuWnTJs/6edGzzPXf13PGC3kHNp4ZJKPvnb6/+gzoUlG933pYjg4Z6FBc2LI6pJfex7D3XJ/7N954w7PeR7Pg7wT9TOhQRDqHT+kQAAAACgIAAGBW51DEHl+qZy5r20pnRzZp0sRzaWlp4Gv0LGg9xENbItoC04NytIWiM6B11yht02Rqlmaq0tFiTed52LWh1xE2sz2shan3WO+Zvibb7ejafj+emcyI0zMTRq+vQYMGnlu3bu1ZPxc6nBY2nJONe5btZyYOdBWR7kppFryv+uzq8KyuSAp7/6PeFzoEAACAggAAAGRwyADRFEP7s1DR/sxPPDP/E+VniPswW9zo8IFZcJhHh+wSNzCqDkMGAAAgMgoCAACQ3rMMAADZwSZQ8aMrBnKBDgEAAKAgAAAAFAQAAMAoCAAAgFEQAAAAS2FjIgAAEF90CAAAAAUBAACgIAAAAEZBAAAAjIIAAAAYBQEAADAKAgAAYBQEAADAKAgAAICZ/RciH81xtMEm8wAAAABJRU5ErkJggg==","text/plain":["<Figure size 640x480 with 10 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# reconstructing training images\n","train_imgs = next(iter(mnist_train_loader))[0]\n","display_ae_images(ae_model, train_imgs)"]},{"cell_type":"markdown","metadata":{"id":"h3XxWKjGKtoA"},"source":["What about images from the test set?"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"ExMLThLofLn2"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 10 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# reconstructing test images\n","test_imgs = next(iter(mnist_test_loader))[0]\n","display_ae_images(ae_model, test_imgs)"]},{"cell_type":"markdown","metadata":{"id":"icWAOpmWKwid"},"source":["There is not too much overfitting at work here apparently. We can quantify this by computing the reconstruction loss over the test dataset (below) and comparing it to the reconstruction loss over the training dataset at the end of training (check the training cell above)."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"HbyzjCqUjHQU"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 157/157 [00:01<00:00, 86.02batch/s, loss=79.1]"]},{"name":"stdout","output_type":"stream","text":["Test Loss: 87.2074\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["ae_model.eval()\n","test_loss = 0.0\n","\n","# We will store all the latent codes corresponding to the test images for reuse\n","# later on.\n","zs_test = np.zeros((len(mnist_test_loader.dataset), ae_model.latent_dim))\n","\n","n = 0\n","with tqdm(mnist_test_loader, unit=\"batch\") as tepoch:\n","    for data, labels in tepoch:\n","        # Put the data on the correct device:\n","        data = data.to(device)\n","\n","        # Pass the data through the model\n","        predict = ae_model(data)\n","        reconstructions = predict[\"reconstructions\"]\n","        z = predict[\"codes\"]\n","\n","        # Compute the AE loss\n","        loss = reconstruction_loss(reconstructions, data)\n","\n","        # Store quantities of interest\n","        minibatch_size = z.shape[0]\n","        zs_test[n : (n + minibatch_size), :] = z.detach().cpu().numpy()\n","\n","        # Compute the loss\n","        test_loss += loss.item()\n","\n","        # tqdm bar displays the loss\n","        tepoch.set_postfix(loss=loss.item())\n","\n","        # increment n to fill next parts of the arrays\n","        n += minibatch_size\n","\n","print(\"Test Loss: {:.4f}\".format(test_loss / len(mnist_test_loader)))"]},{"cell_type":"markdown","metadata":{"id":"assPaJqB5sa-"},"source":["The test and training average reconstruction losses are indeed similar."]},{"cell_type":"markdown","metadata":{"id":"SoLTYl8ELZsj"},"source":["Are you happy with the quality of the __reconstructions__? Next, we will see if this autoencoder model is good at __generating__ images."]},{"cell_type":"markdown","metadata":{},"source":["**Answer**:\n","\n","The quality of the reconstructions seem to be reasonably good, despite the loss of sharpness of the contours and the extremities of high curvature strokes. For some images, however, in which the handwritten digits were already harder to discern, the quality of the reconstruction is worse, albeit still acceptable."]},{"cell_type":"markdown","metadata":{"id":"zO9ATQEiyr3b"},"source":["# 2. Image generation with the vanilla autoencoder\n","\n","Unfortunately, the vanilla autoencoder is not in itself a generative model because it does not define a joint probability distribution of the data and latent codes. We need to come up with roundabout ways to synthetize data based on this model."]},{"cell_type":"markdown","metadata":{"id":"ECzGyqbyMaCt"},"source":["In this section, we consider two naïve approaches to creating generative models from the AE. The general idea is the following:\n","\n","- train an autoencoder\n","- estimate different statistics (mean, variance) of the data in the latent space\n","- using these statistics, define a model based on a Gaussian distribution in the latent space\n","- generate latent codes with this distribution, then decode them back to image space to obtain synthetic images\n","\n","We will consider these two situations :\n","\n","- a multivariate Gaussian distribution with __diagonal covariance matrix__ (each latent variable is an independent random variable). This requires the mean and variance in each latent variable;\n","- a multivariate Gaussian distribution with __non-diagonal covariance matrix__. This requires the mean and covariance matrix of the latent codes.\n","\n","Obviously, since this is done _a posteriori_ after training the autoencoder, there is nothing which guarantees that the latent codes do indeed follow a Gaussian distribution. Our goal will be to verify that Variational Autoencoders indeed produce better results than such naïve approaches."]},{"cell_type":"markdown","metadata":{"id":"x2M1-BRmf56d"},"source":["### 2.0. Defining and generating random Gaussian latent codes\n","\n","Let $z$ be a latent code and $D$ the dimension of the latent space (called ``latent_dim`` in the code). We suppose that the $z$'s follow a multivariate Gaussian distribution, written as:\n","\n","\\begin{equation}\n","z \\sim \\mathcal{N}\\left(\n","\\mu,\n","\\bf{C}\n","\\right),\n","\\end{equation}\n","where $\\mu$ and $\\bf{C}$ are the mean vector and covariance matrix of the Gaussian distribution. To define such a generative model, we must therefore determine $\\mu$ and $\\bf{C}$. Once this is done, we can generate a random Gaussian latent code in the following manner:\n","\n","\\begin{equation}\n","z = \\mu + {\\bf{L}} \\varepsilon,\n","\\end{equation}\n","where $\\varepsilon$ is a random vector drawn from a multivariate normal distribution ($\\mu=0$ and ${\\bf{C}} = \\text{Id}$), and $\\bf{L}$ is output by a Cholesky decomposition of the positive semi-definite covariance matrix. In other words:\n","\n","\\begin{equation}\n","{\\bf{C}} = {\\bf{L}}{\\bf{L}^T}.\n","\\end{equation}\n","\n","This gives a simple method of producing a multivariate Gaussian random variable."]},{"cell_type":"markdown","metadata":{"id":"NWpucm972i7j"},"source":["### 2.1. A Gaussian model with diagonal covariance\n","\n","The first naïve model is  defined in this first case as:\n","\n","- $\\bf{\\mu}=\\left[\\mu_0, \\mu_1, \\cdots, \\mu_{d-1}\\right]^T$\n","- $\n","  \\bf{C} = \\begin{pmatrix}\n","\\sigma_0^2 & 0 & \\cdots & 0 \\\\\n","0 & \\sigma_1^2 & \\cdots & 0 \\\\\n","\\vdots & \\ddots & \\ddots & \\vdots \\\\\n","0 & 0 & \\cdots & \\sigma_{d-1}^2\n","\\end{pmatrix}$\n","\n","In this situation, therefore, the matrix $\\bf{L}$ can be calculated quite simply, as:\n","- $\n","  \\bf{L} = \\begin{pmatrix}\n","\\sigma_0 & 0 & \\cdots & 0 \\\\\n","0 & \\sigma_1 & \\cdots & 0 \\\\\n","\\vdots & \\ddots & \\ddots & \\vdots \\\\\n","0 & 0 & \\cdots & \\sigma_{d-1}\n","\\end{pmatrix}$"]},{"cell_type":"markdown","metadata":{"id":"gZCIY7RsN4NY"},"source":["We are going to compute the mean and the component-wise standard deviations from a batch of data. For simplicity you are going to use the latent codes `zs_test` corresponding to the test data to estimate these quantities.<br>\n","\n","It is actually bad practice, and it would be better to estimate them from the training dataset. We do not do so here for convenience because we have already computed `zs_test` above (we have verified above that overfitting was not a problem, so the difference between the two estimates should be minor)."]},{"cell_type":"code","execution_count":17,"metadata":{"id":"sUXHCtvW2iQ0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Average of latent codes: [  4.2734387   -1.7539146    5.97020288 -15.00411164  13.92918885\n","   4.17385561   1.08394415   3.56108578 -17.96395001 -18.11138416]\n","Standard deviation of latent codes: [5.33803808 4.91536908 6.34686722 7.79089384 6.65241752 6.4884011\n"," 5.59154509 4.76035523 6.84485666 6.19515194]\n"]}],"source":["# zs_test is of shape (N,D) where N is the test dataset size and D the latent dimension\n","# Compute the vector of mean values and the vector of component-wise std's.\n","z_average = np.mean(zs_test, axis=0)\n","z_sigma = np.std(zs_test, axis=0)\n","\n","print(\"Average of latent codes:\", z_average)\n","print(\"Standard deviation of latent codes:\", z_sigma)"]},{"cell_type":"markdown","metadata":{"id":"Lrpc62ML9K4l"},"source":["Now, in the next cell generate data with this simple generative model using the approach described above. Display these images with the `display_images` function.\n","\n","__Hint__. You do not actually have to define the matrix $\\bf{L}$ in this case, an element-wise multiplication of two (properly chosen) vectors will suffice. To generate multivariate normal random variables you can use the following Pytorch function:\n","\n","- `torch.randn`\n","\n","To convert a numpy array to pytorch tensor, use `torch.from_numpy(...).float()`\n"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"cRXyUkVeppif"},"outputs":[],"source":["def generate_images_diagonal_gaussian(ae_model, z_average, z_sigma, n_images=5):\n","    # Sample noise from a standard Gaussian distribution\n","    epsilon = torch.randn(n_images, z_average.shape[0])\n","\n","    # Using epsilon, generate samples from N(mu, C)\n","    z_generated = (\n","        torch.from_numpy(z_average).float()\n","        + torch.from_numpy(z_sigma).float() * epsilon\n","    )\n","\n","    # Decode back to image space\n","    imgs_generated = ae_model.decoder(z_generated.to(device))\n","\n","    return imgs_generated"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"1_Tekii-9QEo"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 5 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["imgs_generated = generate_images_diagonal_gaussian(\n","    ae_model, z_average, z_sigma, n_images=5\n",")\n","display_images(imgs_generated)"]},{"cell_type":"markdown","metadata":{"id":"xiNaEgLIloeA"},"source":["What do you think of these samples? Next let's try a slightly more sophisticated model."]},{"cell_type":"markdown","metadata":{},"source":["**Answer**:\n","\n","The samples seem to be, in general, a scribble which one could probably recognize as a handwritten digit. The quality of the generated images is, however, very poor and cannot be discerned as a proper handwritten digit at times but as an incomprehensible scribble."]},{"cell_type":"markdown","metadata":{"id":"WjVPfkRKYMSh"},"source":["### 2.2. Non-diagonal Gaussian model\n","\n","The second model uses a non-diagonal covariance matrix $\\bf{C}$ in the multivariate Gaussian distribution. In the next cell, calculate the mean and covariance matrix from `zs_test`.\n","\n","__Hint__. You can use the `np.cov` function. Make sure to put the data in the right format for this. The documentation might be incorrect, print the shape of z_covariance to verify that you have a matrix of the correct shape (the covariance matrix and not the Gram matrix)."]},{"cell_type":"code","execution_count":20,"metadata":{"id":"ArXgre39CD2H"},"outputs":[{"name":"stdout","output_type":"stream","text":["Average of latent codes: [  4.2734387   -1.7539146    5.97020288 -15.00411164  13.92918885\n","   4.17385561   1.08394415   3.56108578 -17.96395001 -18.11138416]\n","Covariance matrix of latent codes: [[ 2.84975003e+01 -3.88960167e+00  2.10689909e+01 -7.01329784e+00\n","  -7.18567261e+00 -8.10020198e+00  5.00769306e+00  2.55292162e+00\n","   2.13241802e-01  9.69282444e+00]\n"," [-3.88960167e+00  2.41632695e+01 -7.43515762e-01 -3.37272500e+00\n","  -8.88972627e+00  1.50404089e+01  1.10233536e+00 -2.89710916e+00\n","  -1.62899328e+00  4.04809488e+00]\n"," [ 2.10689909e+01 -7.43515762e-01  4.02867522e+01 -5.63426655e-02\n","  -2.96813071e+00  4.97479264e-01  3.75988667e+00 -3.26463427e+00\n","  -3.59637565e+00  8.13024132e+00]\n"," [-7.01329784e+00 -3.37272500e+00 -5.63426655e-02  6.07040972e+01\n","  -1.54311338e+01 -3.16953356e+00  2.08385211e+01 -9.11449174e+00\n","   1.35104473e+01  1.75825372e+00]\n"," [-7.18567261e+00 -8.88972627e+00 -2.96813071e+00 -1.54311338e+01\n","   4.42590848e+01 -3.12882670e+00 -1.85787743e+01 -1.86839574e-01\n","  -1.80522965e+01 -3.94647718e+00]\n"," [-8.10020198e+00  1.50404089e+01  4.97479264e-01 -3.16953356e+00\n","  -3.12882670e+00  4.21035591e+01  4.08497562e-01 -1.01409261e+01\n","  -3.71480444e+00 -7.13321385e+00]\n"," [ 5.00769306e+00  1.10233536e+00  3.75988667e+00  2.08385211e+01\n","  -1.85787743e+01  4.08497562e-01  3.12685034e+01 -7.02511747e-01\n","   7.72012909e+00  3.88759386e-01]\n"," [ 2.55292162e+00 -2.89710916e+00 -3.26463427e+00 -9.11449174e+00\n","  -1.86839574e-01 -1.01409261e+01 -7.02511747e-01  2.26632482e+01\n","  -7.71061697e+00 -3.11392775e-01]\n"," [ 2.13241802e-01 -1.62899328e+00 -3.59637565e+00  1.35104473e+01\n","  -1.80522965e+01 -3.71480444e+00  7.72012909e+00 -7.71061697e+00\n","   4.68567483e+01 -7.48433301e+00]\n"," [ 9.69282444e+00  4.04809488e+00  8.13024132e+00  1.75825372e+00\n","  -3.94647718e+00 -7.13321385e+00  3.88759386e-01 -3.11392775e-01\n","  -7.48433301e+00  3.83837459e+01]]\n"]}],"source":["z_average = np.mean(zs_test, axis=0)\n","z_covariance = np.cov(zs_test.T)\n","\n","print(\"Average of latent codes:\", z_average)\n","print(\"Covariance matrix of latent codes:\", z_covariance)"]},{"cell_type":"markdown","metadata":{"id":"JhXU8cnTZ0E8"},"source":["Now, generate some samples with this distribution. In this case, you actually have to calculate the Cholesky decomposition and find $\\bf{L}$. For this, you can use `np.linalg.cholesky`. Then compute the latent codes according to $z = \\mu + {\\bf{L}} \\varepsilon$.\n","\n","__Hint__. You can use `torch.matmul`. Pay attention to the dimension of `epsilon` to implement it correctly."]},{"cell_type":"code","execution_count":21,"metadata":{"id":"rSbYZLZGrcdR"},"outputs":[],"source":["# calculate Cholesky decomposition of covariance matrix : C = L L^T\n","L = np.linalg.cholesky(z_covariance)"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"zXGlJTZ7Z4ed"},"outputs":[],"source":["def generate_images_non_diagonal_gaussian(ae_model, z_average, L, n_images=5):\n","    # Generate noise according to a standard Gaussian distribution\n","    epsilon = torch.randn(n_images, z_average.shape[0])\n","\n","    # Sample latent codes using epsilon\n","    z_generated = torch.from_numpy(z_average).float() + torch.matmul(\n","        epsilon, torch.from_numpy(L).float()\n","    )\n","\n","    # Decode back to image space\n","    imgs_generated = ae_model.decoder(z_generated.to(device))\n","\n","    return imgs_generated"]},{"cell_type":"markdown","metadata":{"id":"xs0o_VUvR3jl"},"source":["Generate images using this model now:"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"kGCJVjbXsKLE"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 5 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["imgs_generated = generate_images_non_diagonal_gaussian(\n","    ae_model, z_average, L, n_images=5\n",")\n","display_images(imgs_generated)"]},{"cell_type":"markdown","metadata":{"id":"qLtsdri6zKEm"},"source":["You should see some improvement, but we can do better than this. Thus, we turn to the variational autoencoder."]},{"cell_type":"markdown","metadata":{"id":"8UqeNhuSdnDt"},"source":["# 3. Variational autoencoder\n","\n","Now, we are going to create a variational autoencoder to carry out __image generation__. Let's first recall the idea of a variational autoencoder.\n","\n","### Main idea\n","\n","The main idea is to create an autoencoder whose latent codes follow a certain distribution (a Gaussian distribution in practice). This is done with two tools :\n","\n","- A specific architecture, where the encoder produces the mean and variance of the latent codes\n","- A specially designed loss function\n","\n","Once the VAE is trained, it is possible to sample in the latent space by producing random normal variables and simply decoding.\n","\n","### Architecture\n","\n","The architecture of the VAE model is the same as before (using `Encoder` with `multiplier=2` and `Decoder`). However the wrapper `VAEModel` will be a bit more complex as we need to implement the reparametrization trick. We will also implement the code to generate samples (for test time).\n","\n","### Variational Autoencoder loss\n","\n","The VAE loss consists in a reconstruction loss and a KL divergence term.\n","\n","- The reconstruction loss is the same `reconstruction_loss` as before. In other words, the reconstructions are compared to the input images using binary cross-entropy. The reconstructions are generated by sampling a latent code from $q(z|x)$ and decoding it back to image space.\n","\n","- You will implement the KL divergence term manually below."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"6siMHQLheM4T"},"outputs":[],"source":["class VAEModel(nn.Module):\n","    def __init__(self, latent_dim):\n","        super(VAEModel, self).__init__()\n","\n","        self.latent_dim = latent_dim\n","        self.encoder = Encoder(latent_dim, multiplier=2)\n","        self.decoder = Decoder(latent_dim)\n","\n","    def reparameterize(self, mean, logvar, mode=\"sample\"):\n","        \"\"\"\n","        Samples from a normal distribution using the reparameterization trick.\n","\n","        Parameters\n","        ----------\n","        mean : torch.Tensor\n","            Mean of the normal distribution. Shape (batch_size, latent_dim)\n","\n","        logvar : torch.Tensor\n","            Diagonal log variance of the normal distribution. Shape (batch_size,\n","            latent_dim)\n","\n","        mode : 'sample' or 'mean'\n","            Returns either a sample from qzx, or just the mean of qzx. The former\n","            is useful at training time. The latter is useful at inference time as\n","            the mean is usually used for reconstruction, rather than a sample.\n","        \"\"\"\n","        if mode == \"sample\":\n","            # Implements the reparametrization trick (slide 43):\n","            std = torch.exp(2 * logvar)\n","            eps = torch.randn_like(std)\n","            return mean + std * eps\n","        elif mode == \"mean\":\n","            return mean\n","        else:\n","            return ValueError(\"Unknown mode: {mode}\".format(mode))\n","\n","    def forward(self, x, mode=\"sample\"):\n","        \"\"\"\n","        Forward pass of model, used for training or reconstruction.\n","\n","        Parameters\n","        ----------\n","        x : torch.Tensor\n","            Batch of data. Shape (batch_size, n_chan, height, width)\n","\n","        mode : 'sample' or 'mean'\n","            Reconstructs using either a sample from qzx or the mean of qzx\n","        \"\"\"\n","        # stats_qzx is the output of the encoder\n","        stats_qzx = self.encoder(x)\n","\n","        # Use the reparametrization trick to sample from q(z|x)\n","        samples_qzx = self.reparameterize(*stats_qzx.unbind(-1), mode=mode)\n","\n","        # Decode the samples to image space\n","        reconstructions = self.decoder(samples_qzx)\n","\n","        # Return everything:\n","        return {\n","            \"reconstructions\": reconstructions,\n","            \"stats_qzx\": stats_qzx,\n","            \"samples_qzx\": samples_qzx,\n","        }\n","\n","    def sample_qzx(self, x):\n","        \"\"\"\n","        Returns a sample z from the latent distribution q(z|x).\n","\n","        Parameters\n","        ----------\n","        x : torch.Tensor\n","            Batch of data. Shape (batch_size, n_chan, height, width)\n","        \"\"\"\n","        stats_qzx = self.encoder(x)\n","        samples_qzx = self.reparameterize(*stats_qzx.unbind(-1))\n","        return samples_qzx\n","\n","    def sample_pz(self, N):\n","        samples_pz = torch.randn(\n","            N, self.latent_dim, device=self.encoder.conv1.weight.device\n","        )\n","        return samples_pz\n","\n","    def generate_samples(self, samples_pz=None, N=None):\n","        if samples_pz is None:\n","            if N is None:\n","                return ValueError(\n","                    \"samples_pz and N cannot be set to None at the same time. Specify one of the two.\"\n","                )\n","\n","            # If samples z are not provided, we sample N samples from the prior\n","            # p(z) = N(0,Id), using sample_pz\n","            samples_pz = self.sample_pz(N)\n","\n","        # Decode the z's to obtain samples in image space (here, probability\n","        # maps which can later be sampled from or thresholded)\n","        generations = self.decoder(samples_pz)\n","        return {\"generations\": generations}"]},{"cell_type":"markdown","metadata":{"id":"MUsrzszm-Hnf"},"source":["The KL divergence term is computed as per the regularization term in slide 45 i.e., for each data sample in the mini-batch:\n","$$\\frac{1}{2}\\sum_{j=1}^D (\\mu_j^2 + \\sigma_j^2 - 1 - \\log{\\sigma_j^2})$$"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"-pc40PPM7adL"},"outputs":[],"source":["def kl_normal_loss(mean, logvar):\n","    \"\"\"\n","    Calculates the KL divergence between a normal distribution\n","    with diagonal covariance and a unit normal distribution.\n","\n","    Parameters\n","    ----------\n","    mean : torch.Tensor\n","        Mean of the normal distribution. Shape (batch_size, latent_dim) where\n","        D is dimension of distribution.\n","\n","    logvar : torch.Tensor\n","        Diagonal log variance of the normal distribution. Shape (batch_size,\n","        latent_dim)\n","    \"\"\"\n","    # To be consistent with the reconstruction loss, we take the mean over the\n","    # minibatch (i.e., compute for each sample in the minibatch according to\n","    # the equation above, then take the mean).\n","    latent_kl = torch.mean(\n","        0.5 * torch.sum(mean.pow(2) + torch.exp(logvar) - 1 - logvar, dim=1)\n","    )\n","\n","    return latent_kl"]},{"cell_type":"markdown","metadata":{"id":"WhJbXJ0y_8OI"},"source":["The `BetaVAELoss` puts it all together as per slide 55."]},{"cell_type":"code","execution_count":26,"metadata":{"id":"x_hr2EwiCRSv"},"outputs":[],"source":["class BetaVAELoss(object):\n","    \"\"\"\n","    Compute the Beta-VAE loss\n","\n","    Parameters\n","    ----------\n","        beta: (scalar) the weight assigned to the regularization term\n","    \"\"\"\n","\n","    def __init__(self, beta):\n","        self.beta = beta\n","\n","    def __call__(self, reconstructions, data, stats_qzx):\n","        stats_qzx = stats_qzx.unbind(-1)\n","\n","        # Reconstruction loss\n","        rec_loss = reconstruction_loss(reconstructions, data)\n","\n","        # KL loss\n","        kl_loss = kl_normal_loss(stats_qzx[0], stats_qzx[1])\n","\n","        # Total loss of beta-VAE\n","        loss = rec_loss + self.beta * kl_loss\n","\n","        return loss"]},{"cell_type":"markdown","metadata":{"id":"3RGTCPZXWaz6"},"source":["### Training the Variational Autoencoder"]},{"cell_type":"markdown","metadata":{"id":"hk_9fDIphlsi"},"source":["This follows the traditional pipeline that you are by now familiar with."]},{"cell_type":"code","execution_count":27,"metadata":{"id":"bOdRTeDAMJCO"},"outputs":[],"source":["latent_dim = 10\n","\n","learning_rate = 1e-3\n","n_epoch = 5  # use the same number of epochs as before for fairness"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"xGN3jpxqOOwg"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n"]}],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["vae_model = VAEModel(latent_dim=latent_dim)\n","vae_model = vae_model.to(device)"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"fBVo5s-dQCBb"},"outputs":[],"source":["# To keep it simple, we can leave beta at 1.0 for the beta-VAE loss\n","# Feel free to experiment with it to see different trade-offs between reconstruction\n","# and generation performance.\n","\n","vae_loss = BetaVAELoss(beta=1.0)"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"GiqsBcP7KIT3"},"outputs":[],"source":["# AdamW, with learning rate set to the parameter above and weight decay to 1e-4\n","optimizer = optim.AdamW(vae_model.parameters(), lr=learning_rate, weight_decay=1e-4)"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"z4KHRufgxNmR"},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 0: 100%|██████████| 938/938 [00:16<00:00, 58.31batch/s, loss=155]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 0: Train Loss: 189.6350\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1: 100%|██████████| 938/938 [00:17<00:00, 52.13batch/s, loss=112] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss: 121.1619\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2: 100%|██████████| 938/938 [00:17<00:00, 54.77batch/s, loss=103] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss: 102.4142\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3: 100%|██████████| 938/938 [00:16<00:00, 56.42batch/s, loss=104] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss: 96.7922\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4: 100%|██████████| 938/938 [00:17<00:00, 54.82batch/s, loss=97.3]"]},{"name":"stdout","output_type":"stream","text":["Epoch 4: Train Loss: 94.1070\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["vae_model.train()\n","\n","for epoch in range(0, n_epoch):\n","    train_loss = 0.0\n","\n","    with tqdm(mnist_train_loader, unit=\"batch\") as tepoch:\n","        for data, labels in tepoch:\n","            tepoch.set_description(f\"Epoch {epoch}\")\n","\n","            # Put data on correct device, GPU or CPU\n","            data = data.to(device)\n","\n","            # Pass the input data through the model\n","            predict = vae_model(data)\n","            reconstructions = predict[\"reconstructions\"]\n","            stats_qzx = predict[\"stats_qzx\"]\n","\n","            # Compute the beta-VAE loss\n","            loss = vae_loss(reconstructions, data, stats_qzx)\n","\n","            # Backpropagate\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Aggregate the training loss for display at the end of the epoch\n","            train_loss += loss.item()\n","\n","            # tqdm bar displays the loss\n","            tepoch.set_postfix(loss=loss.item())\n","\n","    print(\n","        \"Epoch {}: Train Loss: {:.4f}\".format(\n","            epoch, train_loss / len(mnist_train_loader)\n","        )\n","    )"]},{"cell_type":"markdown","metadata":{"id":"3DFW0vPRXrSB"},"source":["### Testing the VAE model"]},{"cell_type":"markdown","metadata":{"id":"Who7o-hAXLhl"},"source":["Let's check how well the VAE reconstructs training samples:"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"NiRg43Bgx_MH"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 10 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# reconstructing training images\n","train_imgs = next(iter(mnist_train_loader))[0]\n","display_ae_images(vae_model, train_imgs)"]},{"cell_type":"markdown","metadata":{"id":"Bqa9wmZ3XSxD"},"source":["Same for test samples:"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"IRJ9-V0tx_MJ"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 10 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# reconstructing test images\n","test_imgs = next(iter(mnist_test_loader))[0]\n","display_ae_images(vae_model, test_imgs)"]},{"cell_type":"markdown","metadata":{"id":"OR-uGTX2XaEB"},"source":["Let's compute the quantitative `reconstruction_loss` on the test data:"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"P0EzdVDzx_MJ"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 157/157 [00:01<00:00, 80.50batch/s, loss=77.7]"]},{"name":"stdout","output_type":"stream","text":["Test Loss: 86.2220\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# !!! If you copy paste code, don't forget to change ae_model to vae_model !!!\n","\n","vae_model.eval()\n","test_loss = 0.0\n","\n","n = 0\n","with tqdm(mnist_test_loader, unit=\"batch\") as tepoch:\n","    for data, labels in tepoch:\n","        # Put the data on the correct device:\n","        data = data.to(device)\n","\n","        # Pass the data through the model\n","        predict = vae_model(data)\n","        reconstructions = predict[\"reconstructions\"]\n","\n","        # Compute the AE loss\n","        loss = reconstruction_loss(reconstructions, data)\n","\n","        # Compute the loss\n","        test_loss += loss.item()\n","\n","        # tqdm bar displays the loss\n","        tepoch.set_postfix(loss=loss.item())\n","\n","        # increment n to fill next parts of the arrays\n","        n += minibatch_size\n","\n","print(\"Test Loss: {:.4f}\".format(test_loss / len(mnist_test_loader)))"]},{"cell_type":"markdown","metadata":{"id":"wFOJ0yCzXuxO"},"source":["### Image generation with the VAE model"]},{"cell_type":"markdown","metadata":{"id":"uTfRje_AkKDr"},"source":["Now, generate some images with the VAE model. You can directly use the `generate_samples` routine from the `VAEModel` class above."]},{"cell_type":"code","execution_count":36,"metadata":{"id":"NGt_LSDEE2vz"},"outputs":[],"source":["def generate_images_vae(vae_model, n_images=5):\n","    return vae_model.generate_samples(N=n_images)[\"generations\"]"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"41tXdNsFkKk5"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 5 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["imgs_generated = generate_images_vae(vae_model)\n","\n","display_images(imgs_generated)"]},{"cell_type":"markdown","metadata":{"id":"lGnvKoynzaFN"},"source":["Do you think the results are better ? What advantage does the Variational Autoencoder have over the simple autoencoder model, even though the second (non-diagonal Gaussian model) AE approach has a more complex probabilistic latent model (a full covariance matrix)?"]},{"cell_type":"markdown","metadata":{},"source":["**Answer**:\n","\n","The results of the VAE model strongly depends on the parameter $\\beta$, used on its loss function. Lower $\\beta$ values yield superior reconstructions but lackluster generation quality.  Conversely, higher $\\beta$ values may result in impressive generation but subpar reconstruction.\n","\n","For $\\beta=1.0$: the reconstruction made by the variational auto-encoder are better as, visually, the reconstructed images are almost a perfect match to the input images. The quality of the generation of samples, however, seem visually similar to the previous methods, albeit slightly worse. This illustrates, thus, the necessity of quantitative metrics to evaluate the quality of sample generation."]},{"cell_type":"markdown","metadata":{"id":"uXm-D9Ef9vYm"},"source":["Next, we compare the models quantitavely."]},{"cell_type":"markdown","metadata":{"id":"04MddkzuE324"},"source":["# 4. Evaluating and comparing the models for image generation"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"D8WsuVqmaWmd"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n"]}],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"B93T6PD2YhgF"},"source":["Nowadays, the standard metric to evaluate the generative performance of a model is the FID (Fréchet Inception Distance). I leave it for you as optional homework to implement it if you wish to do so.\n","\n","Here, we will follow another path, a simplified version of the Inception Score (IS) that has been somewhat superseded by the FID:\n","\n","- we train a simple convolutional neural network classifier on MNIST, to a good accuracy\n","- we generate images with each model\n","- we find the average of the highest probability of the images according to the classifier, for each model. If this value is high, it means that on average the classifier considers that the images looks like a real image of an actual digit"]},{"cell_type":"markdown","metadata":{"id":"Fk1OdN6jZZf1"},"source":["We will use the following simple convolutional architecture for the classifier:\n","\n","- conv2d, 3x3 kernel, 32 filters, stride=2, padding=1; + ReLU + BatchNorm2d\n","- conv2d, 3x3 kernel, 64 filters, stride=2, padding=1; + ReLU + BatchNorm2d\n","- conv2d, 3x3 kernel, 128 filters, stride=2, padding=1; + ReLU + BatchNorm2d\n","- Global Average Pooling\n","- Flatten\n","- Dense layer\n","\n","Now, define the model. To make things easier, we use the `torch.nn.Sequential` API to implement the model (there is no need for a Class in this simple case).\n","\n","__Hint__. For the global average pooling, use the `torch.nn.AvgPool2d` layer with the suitable kernel size and stride."]},{"cell_type":"code","execution_count":39,"metadata":{"id":"P87a-DkXFOCv"},"outputs":[],"source":["nb_classes = 10\n","kernel_size = (3, 3)\n","\n","mnist_classification_model = nn.Sequential(\n","    nn.Conv2d(in_channels=1, out_channels=32, kernel_size=kernel_size, stride=2, padding=1),\n","    nn.ReLU(),\n","    nn.BatchNorm2d(32),\n","\n","    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=kernel_size, stride=2, padding=1),\n","    nn.ReLU(),\n","    nn.BatchNorm2d(64),\n","\n","    nn.Conv2d(in_channels=64, out_channels=128, kernel_size=kernel_size, stride=2, padding=1),\n","    nn.ReLU(),\n","    nn.BatchNorm2d(128),\n","\n","    nn.AvgPool2d(kernel_size=(4, 4)),\n","\n","    nn.Flatten(),\n","\n","    nn.Linear(128, nb_classes)\n",")\n","\n","mnist_classification_model = mnist_classification_model.to(device)"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"rJ6ZYkKVaY64"},"outputs":[],"source":["learning_rate = 1e-2\n","n_epoch = 5"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"Zexnteq71bJ5"},"outputs":[],"source":["# Cross entropy with reduction='sum'\n","criterion = nn.CrossEntropyLoss(reduction=\"sum\")"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"pWkFh6zHajBg"},"outputs":[],"source":["# AdamW, weight decay set to 1e-4\n","optimizer = optim.AdamW(mnist_classification_model.parameters(), lr=learning_rate, weight_decay=1e-4)"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"eBZrmVV42Gej"},"outputs":[],"source":["def vector_to_class(x):\n","    y = torch.argmax(F.softmax(x, dim=1), axis=1)\n","    return y"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"aOww0ydr2fT0"},"outputs":[],"source":["def cnn_accuracy(x_pred, x_label):\n","    acc = (x_pred == x_label).sum() / (x_pred.shape[0])\n","    return acc"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"0FA8YoX2FcHP"},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 0: 100%|██████████| 938/938 [00:15<00:00, 59.16batch/s, loss=4.16] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch:0 Train Loss:0.1648 Accuracy:0.9498\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1: 100%|██████████| 938/938 [00:15<00:00, 62.12batch/s, loss=0.644] \n"]},{"name":"stdout","output_type":"stream","text":["Epoch:1 Train Loss:0.0608 Accuracy:0.9813\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2: 100%|██████████| 938/938 [00:15<00:00, 60.61batch/s, loss=1.49]  \n"]},{"name":"stdout","output_type":"stream","text":["Epoch:2 Train Loss:0.0482 Accuracy:0.9847\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3: 100%|██████████| 938/938 [00:15<00:00, 62.08batch/s, loss=0.0618]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch:3 Train Loss:0.0394 Accuracy:0.9873\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4: 100%|██████████| 938/938 [00:15<00:00, 60.21batch/s, loss=0.807] "]},{"name":"stdout","output_type":"stream","text":["Epoch:4 Train Loss:0.0333 Accuracy:0.9892\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["mnist_classification_model.train()\n","\n","for epoch in range(0, n_epoch):\n","    train_loss = 0.0\n","    all_labels = []\n","    all_predicted = []\n","\n","    with tqdm(mnist_train_loader, unit=\"batch\") as tepoch:\n","        for imgs, labels in tepoch:\n","            tepoch.set_description(f\"Epoch {epoch}\")\n","\n","            # Put data on correct device\n","            imgs = imgs.to(device)\n","            labels = labels.to(device)\n","\n","            # forward pass and loss computation\n","            predict = mnist_classification_model(imgs)\n","            loss = criterion(predict, labels)\n","\n","            # backpropagation\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            # compute the train loss\n","            train_loss += loss.item()\n","\n","            # store labels and class predictions\n","            all_labels.extend(labels.tolist())\n","            all_predicted.extend(vector_to_class(predict).tolist())\n","\n","            # tqdm bar displays the loss\n","            tepoch.set_postfix(loss=loss.item())\n","\n","    print(\n","        \"Epoch:{} Train Loss:{:.4f} Accuracy:{:.4f}\".format(\n","            epoch,\n","            train_loss / len(mnist_train_loader.dataset),\n","            cnn_accuracy(np.array(all_predicted), np.array(all_labels)),\n","        )\n","    )"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"5bRM-1_x4bu2"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 157/157 [00:01<00:00, 101.12batch/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Test Accuracy: 0.9836\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["mnist_classification_model.eval()\n","\n","all_predicted = []\n","all_labels = []\n","\n","with tqdm(mnist_test_loader, unit=\"batch\") as tepoch:\n","    for imgs, labels in tepoch:\n","        all_labels.extend(labels.tolist())\n","\n","        imgs = imgs.to(device)\n","        predict = mnist_classification_model(imgs)\n","        all_predicted.extend(vector_to_class(predict).tolist())\n","\n","test_accuracy = cnn_accuracy(np.array(all_predicted), np.array(all_labels))\n","\n","print(\"\\nTest Accuracy:\", test_accuracy)"]},{"cell_type":"markdown","metadata":{"id":"zFgN5LblFwTa"},"source":["### Evaluate the average maximum prediction of the images generated by each generative model (higher is better)\n","\n","Now, we evaluate the models. For each ones, produce a certain number of images, and put those images through the classification network. Then find the maximum class probability of each image, and average it over all the images. We will use this as a metric to evaluate each model.\n","\n","__CAREFUL__: the output of the network does __not__ include the Softmax layer, so you will have to carry it out, with:\n","- ```torch.nn.Softmax()(...)```\n","\n","Define this metric now:"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"lCJ_0qqjOXHT"},"outputs":[],"source":["def generative_model_score(imgs_in, classification_model):\n","    gen_score = torch.mean(\n","        torch.max(F.softmax(classification_model(imgs_in), dim=1), dim=1)[0]\n","    )\n","    return gen_score"]},{"cell_type":"markdown","metadata":{"id":"yGq7YFg51UoP"},"source":["Now, generate some images with each of the three models, and evaluate these models:"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"4-L4u2jhILFx"},"outputs":[{"name":"stdout","output_type":"stream","text":["Diagonal gaussian generative model score :  0.8499258160591125\n","Non diagonal gaussian generative model score :  0.8676588535308838\n","Variational autoencoder model score:  0.8560258746147156\n"]}],"source":["imgs_diagonal_gaussian = generate_images_diagonal_gaussian(\n","    ae_model, z_average, z_sigma, n_images=2000\n",")\n","imgs_non_diagonal_gaussian = generate_images_non_diagonal_gaussian(\n","    ae_model, z_average, L, n_images=2000\n",")\n","imgs_vae = generate_images_vae(vae_model, n_images=2000)\n","\n","# average of maximum of first model\n","diagonal_gaussian_score = float(\n","    generative_model_score(imgs_diagonal_gaussian, mnist_classification_model)\n",")\n","non_diagonal_gaussian_score = float(\n","    generative_model_score(imgs_non_diagonal_gaussian, mnist_classification_model)\n",")\n","vae_gaussian_score = float(generative_model_score(imgs_vae, mnist_classification_model))\n","\n","print(\"Diagonal gaussian generative model score : \", diagonal_gaussian_score)\n","print(\"Non diagonal gaussian generative model score : \", non_diagonal_gaussian_score)\n","print(\"Variational autoencoder model score: \", vae_gaussian_score)"]},{"cell_type":"markdown","metadata":{"id":"sxvsG8FC1gNS"},"source":["Questions:\n","\n","- Which model is better quantitatively ? (unfortunately there is some variability, even with 2000 samples; you might want to rerun the cell several times to get the trend)\n","- Do the quantitative result support the qualitative results ?\n","- Can you see any drawbacks of this method of evaluation ?\n","- Can you propose any more sophisticated models than the multivariate Gaussian approach (apart from the variational autoencoder) ?"]},{"cell_type":"markdown","metadata":{},"source":["**Answers**:\n","\n","- When it comes to image generation, the non-diagonal Gaussian generative model typically outperforms its diagonal counterpart. However, the efficacy of the variational autoencoder model is contingent upon the parameter $\\beta$. Lower $\\beta$ values yield superior reconstructions but lackluster generation quality. Conversely, higher $\\beta$ values may result in impressive generation but subpar reconstruction.\n","- The quantitative findings corroborate the qualitative observations, demonstrating that images produced by the non-diagonal Gaussian model exhibit greater clarity and definition compared to its diagonal equivalent. Regarding the variational autoencoder model, outcomes are dictated by the parameter $\\beta$. Lower $\\beta$ values yield images resembling random scribbles with minimal resemblance to handwritten numbers. Conversely, higher $\\beta$ values surpass the quality of images generated by the non-diagonal Gaussian model.\n","- Using the mean maximum class probability obtained by passing generated images through a classification network provides a straightforward metric for evaluating image quality. However, this approach has several limitations. Firstly, it indirectly assesses image quality by focusing solely on the network's classification performance, potentially overlooking other important aspects like clarity and coherence. Secondly, its effectiveness is contingent on the specificity of the dataset and the classification network chosen, limiting its generalizability. Additionally, this method may introduce bias by prioritizing certain characteristics over others and is subject to the subjectivity of network architecture selection and hyperparameter tuning, which were defined for the classification of real images, not generated ones. While useful as part of a comprehensive evaluation strategy, reliance solely on this metric may not capture the full spectrum of image quality.\n","  - Alternatively, one could use the Fréchet Inception Distance (FID). Using FID as an evaluation metric offers significant advantages over classification-based approaches. FID provides a direct measure of image quality by comparing the distributions of feature representations between real and generated images, making it versatile across different datasets and tasks. It considers both the quality and diversity of generated images, producing a single scalar value for quantitative comparison. Moreover, FID is less susceptible to bias introduced by specific network choices and is widely accepted as a benchmark in the research community, making it a robust and reliable metric for evaluating generative models.\n","- Generative Adversarial Networks (GANs) represent a groundbreaking approach to generative modeling, consisting of two neural networks engaged in a dynamic adversarial training process. The generator network learns to produce realistic data samples, such as images, by mapping random noise to the data space, while the discriminator network learns to distinguish between real and generated samples. Through adversarial training, where the generator aims to fool the discriminator and the discriminator aims to accurately distinguish real from fake samples, GANs progressively improve both the quality and diversity of generated outputs. GANs have demonstrated remarkable success in generating high-fidelity images, audio, and text, among other data types, and have found applications in diverse fields including computer vision, natural language processing, and drug discovery. Despite their impressive capabilities, GAN training can be challenging due to issues such as mode collapse, instability, and convergence difficulties, requiring careful architectural design and training strategies. Nonetheless, GANs continue to push the boundaries of generative modeling and hold great promise for advancing artificial intelligence and creative applications in the future."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
